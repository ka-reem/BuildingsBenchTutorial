{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3507384-4705-4d45-adeb-c29c7f858a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from os import environ\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201cce08-7153-4a7b-8e92-5f78e79f8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Handler ===\n",
    "class DataHandler:\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def load_dataset(self, dataset_name, scaler_transform):\n",
    "        from buildings_bench import load_torch_dataset\n",
    "        return list(load_torch_dataset(\n",
    "            dataset_name,\n",
    "            apply_scaler_transform=scaler_transform,\n",
    "            scaler_transform_path=Path(environ[\"TRANSFORM_PATH\"])\n",
    "        ))\n",
    "\n",
    "    def create_dataloader(self, dataset):\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56299e4-ff36-462d-9447-5d504b2584fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Base Model and Subclasses ===\n",
    "\n",
    "class Model(nn.Module):\n",
    "    DEFAULT_CONTEXT_LEN = 168\n",
    "    DEFAULT_PRED_LEN = 24\n",
    "\n",
    "    def __init__(self, activation):\n",
    "        super().__init__()\n",
    "        self.context_len = self.DEFAULT_CONTEXT_LEN\n",
    "        self.pred_len = self.DEFAULT_PRED_LEN\n",
    "        self.activation = self._get_activation(activation)\n",
    "        self.embeddings = self._create_embeddings()\n",
    "\n",
    "    def _create_embeddings(self):\n",
    "        return nn.ModuleDict({\n",
    "            'power': nn.Linear(1, 64),\n",
    "            'building': nn.Embedding(2, 32),\n",
    "            'lat': nn.Linear(1, 32),\n",
    "            'lon': nn.Linear(1, 32)\n",
    "        })\n",
    "\n",
    "    def _get_activation(self, name):\n",
    "        return {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"leaky_relu\": nn.LeakyReLU()\n",
    "        }.get(name.lower(), nn.ReLU())\n",
    "\n",
    "    def _data_pre_process(self, x):\n",
    "        lat = self.embeddings['lat'](x['latitude'])\n",
    "        lon = self.embeddings['lon'](x['longitude'])\n",
    "        btype = self.embeddings['building'](x['building_type'].squeeze(-1))\n",
    "        load = self.embeddings['power'](x['load'])\n",
    "        return torch.cat([lat, lon, btype, load], dim=2)\n",
    "\n",
    "\n",
    "class NN(Model):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__(activation)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_dim = self.context_len * 160\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), self.activation,\n",
    "            nn.Linear(512, 256), self.activation,\n",
    "            nn.Linear(256, 128), self.activation,\n",
    "            nn.Linear(128, self.pred_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        x_flat = ts_embed[:, :self.context_len, :].reshape(x['load'].shape[0], -1)\n",
    "        return self.model(x_flat).unsqueeze(-1)\n",
    "\n",
    "\n",
    "class RNN(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.rnn1, self.rnn2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        rnn1 = nn.RNN(160, 128, batch_first=True)\n",
    "        rnn2 = nn.RNN(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return rnn1, rnn2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.rnn1(ts_embed)\n",
    "        out2, _ = self.rnn2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)\n",
    "\n",
    "\n",
    "class LSTM(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.lstm1, self.lstm2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        lstm1 = nn.LSTM(160, 128, batch_first=True)\n",
    "        lstm2 = nn.LSTM(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return lstm1, lstm2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.lstm1(ts_embed)\n",
    "        out2, _ = self.lstm2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)\n",
    "\n",
    "\n",
    "class GRU(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.gru1, self.gru2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        gru1 = nn.GRU(160, 128, batch_first=True)\n",
    "        gru2 = nn.GRU(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return gru1, gru2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.gru1(ts_embed)\n",
    "        out2, _ = self.gru2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e47687-4fc0-4c80-903c-94ebd54a966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DDP-Compatible Trainer Class ===\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model_name, device, scaler_transform, rank, world_size,\n",
    "                 activation='relu', optimizer_name='adam', lr=1e-3):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.scaler_transform = scaler_transform\n",
    "        self.activation = activation\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.lr = lr\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        self._setup_distributed()\n",
    "        self.model = self._load_model()\n",
    "        self.optimizer = self._get_optimizer()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.handler = DataHandler(batch_size=config[\"batch_size\"])\n",
    "\n",
    "    def _setup_distributed(self):\n",
    "        dist.init_process_group(backend=\"nccl\", rank=self.rank, world_size=self.world_size)\n",
    "\n",
    "    def _load_model(self):\n",
    "        model_map = {'NN': NN, 'RNN': RNN, 'LSTM': LSTM, 'GRU': GRU}\n",
    "        model = model_map[self.model_name](activation=self.activation).to(self.device)\n",
    "        return DDP(model, device_ids=[self.rank])\n",
    "\n",
    "    def _get_optimizer(self):\n",
    "        opt_map = {\n",
    "            'adam': torch.optim.Adam,\n",
    "            'sgd': torch.optim.SGD,\n",
    "            'adamw': torch.optim.AdamW\n",
    "        }\n",
    "        return opt_map.get(self.optimizer_name.lower(), torch.optim.Adam)(\n",
    "            self.model.parameters(), lr=self.lr\n",
    "        )\n",
    "\n",
    "    def _get_ddp_dataloader(self, dataset):\n",
    "        sampler = DistributedSampler(dataset, num_replicas=self.world_size, rank=self.rank, shuffle=True)\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "    def train(self, train_buildings, epochs=5):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for building_id, building_dataset in train_buildings:\n",
    "                dataloader = self._get_ddp_dataloader(building_dataset)\n",
    "                for batch in dataloader:\n",
    "                    for key, value in batch.items():\n",
    "                        batch[key] = value.to(self.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    predictions = self.model(batch)\n",
    "                    targets = batch['load'][:, self.model.module.context_len:, 0]\n",
    "                    loss = self.loss_fn(predictions[:, :, 0], targets)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "            if self.rank == 0:\n",
    "                print(f\"[{self.model_name}] Epoch {epoch + 1}: Loss = {total_loss:.4f}\")\n",
    "        self.model.eval()\n",
    "\n",
    "    def evaluate(self, test_buildings):\n",
    "        self.model.eval()\n",
    "        results = {}\n",
    "        mae_total = 0.0\n",
    "        rmse_total = 0.0\n",
    "        r2_total = 0.0\n",
    "        count = 0\n",
    "        for building_id, building_dataset in test_buildings:\n",
    "            inverse_transform = building_dataset.datasets[0].load_transform.undo_transform\n",
    "            dataloader = self._get_ddp_dataloader(building_dataset)\n",
    "            target_list = []\n",
    "            prediction_list = []\n",
    "            with torch.no_grad():\n",
    "                for batch in dataloader:\n",
    "                    for key, value in batch.items():\n",
    "                        batch[key] = value.to(self.device)\n",
    "                    predictions = self.model(batch)\n",
    "                    targets = batch['load'][:, self.model.module.context_len:]\n",
    "                    targets = inverse_transform(targets)\n",
    "                    predictions = inverse_transform(predictions)\n",
    "                    prediction_list.append(predictions.detach().cpu())\n",
    "                    target_list.append(targets.detach().cpu())\n",
    "            predictions_all = torch.cat(prediction_list)\n",
    "            targets_all = torch.cat(target_list)\n",
    "            mae = torch.abs(predictions_all - targets_all).mean().item()\n",
    "            rmse = torch.sqrt(((predictions_all - targets_all) ** 2).mean()).item()\n",
    "            r2 = 1 - (((predictions_all - targets_all) ** 2).sum() / ((targets_all - targets_all.mean()) ** 2).sum()).item()\n",
    "            mae_total += mae\n",
    "            rmse_total += rmse\n",
    "            r2_total += r2\n",
    "            count += 1\n",
    "            results[building_id] = (predictions_all, targets_all)\n",
    "        return results, mae_total / count, rmse_total / count, r2_total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d545007-d086-4196-8157-8f691725b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Main (SLURM-Aware) DDP Launcher ===\n",
    "\n",
    "def run_ddp(rank, world_size):\n",
    "    os.environ[\"MASTER_ADDR\"] = os.environ.get(\"SLURM_LAUNCH_NODE_IPADDR\", \"127.0.0.1\")\n",
    "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "    torch.cuda.set_device(rank)\n",
    "    device = torch.device(f\"cuda:{rank}\")\n",
    "\n",
    "    # Set environment variables for dataset access\n",
    "    environ[\"PATH\"] = config[\"PATH\"]\n",
    "    environ[\"REPO_PATH\"] = f\"{config['PATH']}/BuildingsBenchTutorial/BuildingsBench/\"\n",
    "    environ[\"BUILDINGS_BENCH\"] = f\"{config['PATH']}/Dataset\"\n",
    "    environ[\"TRANSFORM_PATH\"] = f\"{config['PATH']}/Dataset/metadata/transforms\"\n",
    "\n",
    "    # Dataset loading\n",
    "    handler = DataHandler(batch_size=config[\"batch_size\"])\n",
    "    all_buildings = handler.load_dataset(config[\"dataset_name\"], config[\"scaler_transform\"])\n",
    "    train_buildings = all_buildings[:int(0.8 * len(all_buildings))]\n",
    "    test_buildings = all_buildings[int(0.8 * len(all_buildings)):]\n",
    "\n",
    "    for model_class in [NN, RNN, LSTM, GRU]:\n",
    "        if rank == 0:\n",
    "            print(f\"\\n--- Training {model_class.__name__} ---\")\n",
    "        trainer = Trainer(\n",
    "            model_name=model_class.__name__,\n",
    "            device=device,\n",
    "            scaler_transform=config[\"scaler_transform\"],\n",
    "            activation=config[\"activation\"],\n",
    "            optimizer_name=config[\"optimizer_name\"],\n",
    "            lr=config[\"lr\"],\n",
    "            rank=rank,\n",
    "            world_size=world_size\n",
    "        )\n",
    "        trainer.train(train_buildings, epochs=config[\"epochs\"])\n",
    "        if rank == 0:\n",
    "            _, mae, rmse, r2 = trainer.evaluate(test_buildings)\n",
    "            print(f\"[{model_class.__name__}] MAE: {mae:.4f}, RMSE: {rmse:.4f}, RÂ²: {r2:.4f}\")\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def main():\n",
    "    world_size = int(os.environ[\"SLURM_NTASKS\"])\n",
    "    rank = int(os.environ[\"SLURM_PROCID\"])\n",
    "    run_ddp(rank, world_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BuildingsBenchKernel",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
