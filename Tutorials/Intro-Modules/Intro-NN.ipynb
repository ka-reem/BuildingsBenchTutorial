{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "522cbcdd-d82c-4418-b9b0-cda73d91576a",
   "metadata": {},
   "source": [
    "# Neural Network 101\n",
    "\n",
    "The effectiveness of a deep learning model depends on several factors. These include the choice of architecture, the number of hidden layers, the number of neurons in each layer, the optimizer and activation functions used, and the number of training epochs.\n",
    "\n",
    "While research literature and factors such as dataset type can help guide the selection of an appropriate model architecture, choosing other factors like the number of layers or training epochs typically requires a trial and error process.\n",
    "\n",
    "In this tutorial, you will get hands-on experience with this process. For simplicity, we will restrict the model architecture to a neural network (NN). However, you will have the opportunity to experiment with different activation functions, optimizers, network configurations, and training epochs.\n",
    "\n",
    "Below, you will find simplified definitions of important terminology. We have intentionally kept the explanations non-technical, as theoretical exploration is beyond the scope of this project. However, if you are interested in learning more or gaining deeper insight, please refer to this link `https://www.geeksforgeeks.org/machine-learning/neural-networks-a-beginners-guide/` for further exploration.\n",
    "\n",
    "\n",
    "1) __NN__: A neural network (NN) is a deep learning model architecture inspired by the structure and functioning of the human brain. It consists of artificial neurons, which resemble biological brain cells. These neurons are organized into layers and are interconnected, mimicking the way brain cells are connected in the nervous system.\n",
    "\n",
    "3) __Hidden Layer__: A neural network (NN) consists of three or more layers of neurons. The first and last layers are called the input and output layers, respectively. The layers between them are known as hidden layers (Layers with neurons highlighted in red).\n",
    "\n",
    "![NN model architecture](../../Images/NN.png)\n",
    "\n",
    "3) __Activation Function__: One of the key reasons for the widespread adoption of neural networks is their ability to capture non-linear patterns in data. This capability is made possible through the use of activation functions, which introduce non-linearity into the model.\n",
    "\n",
    "4) __Optimizer__: Optimizer algorithms help a model improve its performance by reducing the error between its predicted and target values.\n",
    "\n",
    "5) __Epochs__: An epoch is one complete pass through the entire training dataset by the model during training.\n",
    "\n",
    "6) __Mean Squared Error__: The average of the squared differences between predicted and actual values. This metric penalizes larger errors more heavily, making it sensitive to outliers. A lower MSE indicates fewer large errors and smaller overall prediction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efbcffba-b89c-4160-b314-922c6b79915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DatasetClass(Dataset):\n",
    "    def __init__(self, num_samples=300):\n",
    "        noise_std=0.8\n",
    "        seed=42\n",
    "        np.random.seed(seed)\n",
    "        self.x = np.sort(np.random.uniform(-3.5, 3.5, num_samples)).astype(np.float32)\n",
    "\n",
    "        y = (\n",
    "            np.sin(7 * self.x ** 3)\n",
    "            + np.log(np.abs(self.x) + 1) * np.cos(3 * self.x ** 2)\n",
    "            + np.tanh(self.x ** 4)\n",
    "        ) * np.exp(-self.x ** 2 / 2) + 0.3 * self.x ** 5\n",
    "\n",
    "        y += np.random.normal(0, noise_std, size=self.x.shape)\n",
    "\n",
    "        self.x = torch.tensor(self.x, dtype=torch.float32).unsqueeze(1)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "class ModelClass(nn.Module):\n",
    "    def __init__(self, model_block: nn.Sequential, activation: str = \"relu\"):\n",
    "        super().__init__()\n",
    "        self.model = model_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class TrainClass:\n",
    "    def __init__(self, model, dataset, optimizer_name=\"adam\", lr=0.01, epochs=30, batch_size=32, test_size=0.2):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optimizer = self._get_optimizer(optimizer_name, model.parameters(), lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=50, gamma=0.5)\n",
    "        self.train_loader, self.test_loader = self._split_dataset(dataset, test_size, batch_size)\n",
    "        self.train_losses = []\n",
    "\n",
    "    def _get_optimizer(self, name, parameters, lr):\n",
    "        name = name.lower()\n",
    "        if name == \"adam\":\n",
    "            return torch.optim.Adam(parameters, lr=lr)\n",
    "        elif name == \"sgd\":\n",
    "            return torch.optim.SGD(parameters, lr=lr)\n",
    "        elif name == \"adamw\":\n",
    "            return torch.optim.AdamW(parameters, lr=lr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {name}\")\n",
    "\n",
    "    def _split_dataset(self, dataset, test_size, batch_size):\n",
    "        indices = np.arange(len(dataset))\n",
    "        train_idx, test_idx = train_test_split(indices, test_size=test_size, random_state=42)\n",
    "        train_set = Subset(dataset, train_idx)\n",
    "        test_set = Subset(dataset, test_idx)\n",
    "        return (\n",
    "            DataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
    "            DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            for xb, yb in self.train_loader:\n",
    "                pred = self.model(xb)\n",
    "                loss = self.loss_fn(pred, yb)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            self.scheduler.step()\n",
    "            avg_loss = total_loss / len(self.train_loader)\n",
    "            self.train_losses.append(avg_loss)\n",
    "\n",
    "            if epoch % 5 == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch} | Train Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        preds, targets = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.test_loader:\n",
    "                pred = self.model(xb)\n",
    "                loss = self.loss_fn(pred, yb)\n",
    "                total_loss += loss.item()\n",
    "                preds.append(pred)\n",
    "                targets.append(yb)\n",
    "\n",
    "        final_mse = total_loss / len(self.test_loader)\n",
    "        print(f\"\\nFinal Test MSE: {final_mse:.6f}\")\n",
    "        return torch.cat(preds), torch.cat(targets)\n",
    "\n",
    "class PlotClass:\n",
    "    @staticmethod\n",
    "    def loss(train_losses):\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(train_losses, label=\"Train Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"MSE Loss\")\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def predictions(model, dataset):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x_all = dataset.x\n",
    "            y_true = dataset.y\n",
    "            y_pred = model(x_all)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.scatter(x_all.numpy(), y_true.numpy(), s=10, label=\"True\", alpha=0.5)\n",
    "        plt.plot(x_all.numpy(), y_pred.numpy(), color=\"red\", label=\"Predicted\", linewidth=2)\n",
    "        plt.title(\"Regression Fit with Noise\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4216117-1de6-4dd5-a961-1f73fecebc3d",
   "metadata": {},
   "source": [
    "## Excersice 1:\n",
    "\n",
    "In this exercise, there are 12 possible combinations of activation functions (relu, tanh, leaky_relu, gelu) and optimizers (adam, sgd, adamw). Randomly select five of these combinations. For each one, train the model for 30 epochs and record both the training loss and final test MSE. Identify which combination results in the lowest final test MSE and training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3516d2-8d06-4560-be63-8edd587299e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # TODO: sSelect five of these combinations activationa and \n",
    "    activation = \"relu\"     # Options: relu, tanh, leaky_relu, gelu\n",
    "    optimizer = \"adam\"      # Options: adam, sgd, adamw\n",
    "    epochs = 30\n",
    "\n",
    "    act_map = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"leaky_relu\": nn.LeakyReLU(),\n",
    "            \"gelu\": nn.GELU()\n",
    "        }\n",
    "    act_ftn = act_map[activation.lower()] \n",
    "    \n",
    "    model_block = nn.Sequential(\n",
    "        nn.Linear(1, 16),\n",
    "        act_ftn,\n",
    "        nn.Linear(16, 4),\n",
    "        act_ftn,\n",
    "        nn.Linear(4, 1)\n",
    "    )\n",
    "    \n",
    "\n",
    "    dataset = DatasetClass()\n",
    "    model = ModelClass(model_block=model_block, activation=activation)\n",
    "    trainer = TrainClass(model, dataset, optimizer_name=optimizer, epochs=epochs)\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "    PlotClass.loss(trainer.train_losses)\n",
    "    PlotClass.predictions(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a972f8-9851-4d4c-876f-2fbe290aa91e",
   "metadata": {},
   "source": [
    "## Excersice 2:\n",
    "Currently, the model architecture is defined as follows: the input layer is nn.Linear(1, 16), the hidden layer is nn.Linear(16, 4), and the output layer is nn.Linear(4, 1).\n",
    "\n",
    "```\n",
    "model_block = nn.Sequential(\n",
    "    nn.Linear(1, 16),\n",
    "    act_ftn,\n",
    "    nn.Linear(16, 4),\n",
    "    act_ftn,\n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "```\n",
    "\n",
    "If you want to add an additional hidden layer, you can modify the architecture like this:\n",
    "```\n",
    "model_block = nn.Sequential(\n",
    "    nn.Linear(1, 16),\n",
    "    act_ftn,\n",
    "    nn.Linear(16, 8), \n",
    "    act_ftn,\n",
    "    nn.Linear(8, 4),\n",
    "    act_ftn,\n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "```\n",
    "\n",
    "Notice that the output dimension of one layer must match the input dimension of the next layer. For example, the first hidden layer outputs 16 values, so the next layer must accept 16 as its input dimension. If these dimensions do not align, you will encounter a RuntimeError.\n",
    "\n",
    "\n",
    "In this exercise, add two additional hidden layers to the model architecture shown below. Then, use the best combination of optimizer and activation function identified in Exercise 1, and train the updated model on 20 epochs.\n",
    "\n",
    "```\n",
    "model_block = nn.Sequential(\n",
    "    nn.Linear(1, 128),\n",
    "    act_ftn,\n",
    "    nn.Linear(128, 16),\n",
    "    act_ftn,\n",
    "    nn.Linear(16, 1)\n",
    ")\n",
    "```\n",
    "\n",
    "## Excersice 3:\n",
    "I was able to achieve a final test MSE of 3.446659 in 30 epochs. Can you build a better model using any combination of optimizer and activation function and beat my result within the same epoch limit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ae030-f457-4801-aca3-1d1367d28be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    activation = \"relu\"     # Options: relu, tanh, leaky_relu, gelu\n",
    "    optimizer = \"adam\"      # Options: adam, sgd, adamw\n",
    "    epochs = 30\n",
    "\n",
    "    act_map = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"leaky_relu\": nn.LeakyReLU(),\n",
    "            \"gelu\": nn.GELU()\n",
    "        }\n",
    "    act_ftn = act_map[activation.lower()] \n",
    "    \n",
    "    model_block = nn.Sequential(\n",
    "        nn.Linear(1, 16),\n",
    "        act_ftn,\n",
    "        nn.Linear(16, 4),\n",
    "        act_ftn,\n",
    "        nn.Linear(4, 1)\n",
    "    )\n",
    "    \n",
    "\n",
    "    dataset = DatasetClass()\n",
    "    model = ModelClass(model_block=model_block, activation=activation)\n",
    "    trainer = TrainClass(model, dataset, optimizer_name=optimizer, epochs=epochs)\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "    PlotClass.loss(trainer.train_losses)\n",
    "    PlotClass.predictions(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c754b272-1a5b-4e1c-9433-fb79589a6038",
   "metadata": {},
   "source": [
    "#### Next Step: `/BuildingsBenchTutorial/Tutorials/Final-Project-Modules/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BuildingsBenchKernel",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
