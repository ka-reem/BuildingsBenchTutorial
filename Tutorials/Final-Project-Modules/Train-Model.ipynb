{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b16fed-8fa6-4ae7-afcf-d5263fabf1d8",
   "metadata": {},
   "source": [
    "# Train Deep Learning Model\n",
    "\n",
    "This notebook consists of two tasks. For both tasks, you must provide your assigned dataset name in list format, just as you did in the previous exercise.\n",
    "\n",
    "When you train a model, it will print the following evaluation metrics:\n",
    "- `Training loss`\n",
    "- `Training time`\n",
    "- `Mean Absolute Error (MAE)`\n",
    "- `Root Mean Squared Error (RMSE)`\n",
    "- `R² score`\n",
    "\n",
    "Below is an overview of the three metrics:\n",
    "\n",
    "### Evaluation Metrics and Their Desired Trends\n",
    "\n",
    "| Metric             | Definition                                                                                  | Desired Trend                                  |\n",
    "|--------------------|----------------------------------------------------------------------------------------------|------------------------------------------------|\n",
    "| MAE (Mean Absolute Error) | Measures the average absolute difference between predicted and actual values.           | ↓ Decrease — Lower MAE indicates better average accuracy. |\n",
    "| RMSE (Root Mean Squared Error)  | Measures the average of squared differences; penalizes larger errors more heavily.      | ↓ Decrease — Lower RMSE indicates fewer large errors.       |\n",
    "| R² (Coefficient of Determination) | Indicates the proportion of variance explained by the model (maximum = 1.0).        | ↑ Increase — Higher R² means better model fit.             |\n",
    "\n",
    "\n",
    "Please familiarize yourself with the metrics above, as they will be important for completing the analysis in the next tutorial.\n",
    "\n",
    "After model evaluation is complete, a folder will be created in your current directory using the following structure:\n",
    "`dataset_name → model_name → activation_function → optimizer_name → epoch_num`\n",
    "\n",
    "Three JSON files will be generated within this directory structure: \n",
    "1) `predictions.json`\n",
    "2) `evaluation.json`\n",
    "3) `train_loss.json`\n",
    "\n",
    "The predictions.json file stores the input data points used for forecasting, along with the predicted values and corresponding target values. The evaluation.json file contains the evaluation metrics, while the train_loss.json file records the training loss and model training time.\n",
    "\n",
    "## Task 1: Train Predefined Models\n",
    "\n",
    "This task involves training four predefined model architectures using the dataset you have been assigned. These models include:\n",
    "\n",
    "* `NN`\n",
    "* `RNN` (Recurrent Neural Network)\n",
    "* `LSTM` (Long Short-Term Memory)\n",
    "* `GRU` (Gated Recurrent Unit)\n",
    "\n",
    "Note: `LSTM` and `GRU` are specialized types of `RNNs` designed to handle sequence data more effectively.\n",
    "\n",
    "You are not required to train all combinations — there are 125 or more available — but you should train at least 30 different combinations. Make sure these include at least four combinations for each model architecture (`NN`, `RNN`, `LSTM`, and `GRU`) to ensure broad coverage.\n",
    "\n",
    "Example Model Combinations\n",
    "```\n",
    "| Model | Activation Function | Optimizer | Epochs |\n",
    "|-------|---------------------|-----------|--------|\n",
    "| NN    | ReLU                | AdamW     | 10     |\n",
    "| RNN   | Tanh                | SGD       | 5      |\n",
    "| LSTM  | GELU                | Adam      | 10     |\n",
    "| GRU   | Leaky ReLU          | AdamW     | 15     |\n",
    "```\n",
    "\n",
    "The code is configured to run over 125 model combinations, which may take significant time to complete.\n",
    "\n",
    "To speed up development and avoid long runtimes, you can reduce the number of combinations by limiting the range of options.\n",
    "\n",
    "Shrinked Option Set (Example 1)\n",
    "```\n",
    "model_classes = [\"NN\", \"RNN\"]\n",
    "activations = [\"relu\", \"tanh\"]\n",
    "optimizers = [\"adam\"]\n",
    "epoch_options = [5, 10]\n",
    "```\n",
    "\n",
    "Shrinked Option Set (Example 2 — Quick Debug)\n",
    "```\n",
    "model_classes = [\"NN\"]\n",
    "activations = [\"relu\"]\n",
    "optimizers = [\"adam\"]\n",
    "epoch_options = [5]\n",
    "```\n",
    "\n",
    "Note: Do not attempt to train for more than 30 epochs per combination, as this may lead to long execution times and unnecessary resource usage.\n",
    "\n",
    "## Task 2: Build and Train Your Custom Model\n",
    "\n",
    "This task focuses on building and training your own custom model, referred to as `MyNN`.\n",
    "\n",
    "In this task, you will design your own model architecture and evaluate its performance by training it under different settings. You should train at least `10` different combinations using `MyNN`.\n",
    "\n",
    "Example Combinations with `MyNN`\n",
    "```\n",
    "| Model | Activation Function | Optimizer | Epochs |\n",
    "|-------|---------------------|-----------|--------|\n",
    "| MyNN  | ReLU                | AdamW     | 10     |\n",
    "| MyNN  | Tanh                | SGD       | 5      |\n",
    "| MyNN  | GELU                | Adam      | 10     |\n",
    "| MyNN  | Leaky ReLU          | AdamW     | 15     |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e715fda-612f-45ef-aa4e-8a953b5a2d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from buildings_bench import load_torch_dataset\n",
    "from buildings_bench.models import model_factory\n",
    "\n",
    "import tomli\n",
    "from pathlib import Path\n",
    "import os \n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "class DataHandler:\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def load_dataset(self, dataset_name, scaler_transform):\n",
    "        from buildings_bench import load_torch_dataset\n",
    "        return list(load_torch_dataset(\n",
    "            dataset_name,\n",
    "            apply_scaler_transform=scaler_transform,\n",
    "            scaler_transform_path=Path(os.environ[\"TRANSFORM_PATH\"])\n",
    "        ))\n",
    "\n",
    "    def create_dataloader(self, dataset):\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "class TimeSeriesSinusoidalPeriodicEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([torch.sin(torch.pi * x), torch.cos(torch.pi * x)], dim=2)\n",
    "        return self.linear(x)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    DEFAULT_CONTEXT_LEN = 168\n",
    "    DEFAULT_PRED_LEN = 24\n",
    "\n",
    "    def __init__(self, activation):\n",
    "        super().__init__()\n",
    "        self.context_len = self.DEFAULT_CONTEXT_LEN\n",
    "        self.pred_len = self.DEFAULT_PRED_LEN\n",
    "        self.activation = self._get_activation(activation)\n",
    "        self.embeddings = self._create_embeddings()\n",
    "\n",
    "    def _create_embeddings(self):\n",
    "        return nn.ModuleDict({\n",
    "            'power': nn.Linear(1, 64),\n",
    "            'building': nn.Embedding(2, 32),\n",
    "            'lat': nn.Linear(1, 32),\n",
    "            'lon': nn.Linear(1, 32), \n",
    "            'day_of_year': TimeSeriesSinusoidalPeriodicEmbedding(32),\n",
    "            'day_of_week': TimeSeriesSinusoidalPeriodicEmbedding(32),\n",
    "            'hour_of_day': TimeSeriesSinusoidalPeriodicEmbedding(32)\n",
    "        })\n",
    "\n",
    "    def _get_activation(self, name):\n",
    "        return {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"leaky_relu\": nn.LeakyReLU()\n",
    "        }.get(name.lower(), nn.ReLU())\n",
    "\n",
    "    def _data_pre_process(self, x):\n",
    "        lat = self.embeddings['lat'](x['latitude'])\n",
    "        lon = self.embeddings['lon'](x['longitude'])\n",
    "        btype = self.embeddings['building'](x['building_type'].squeeze(-1))\n",
    "        load = self.embeddings['power'](x['load'])\n",
    "        day_of_year = self.embeddings['day_of_year'](x['day_of_year'])            \n",
    "        day_of_week = self.embeddings['day_of_week'](x['day_of_week'])            \n",
    "        hour_of_day = self.embeddings['hour_of_day'](x['hour_of_day']) \n",
    "        return torch.cat([lat, lon, btype, day_of_year, day_of_week, hour_of_day, load], dim=2)\n",
    "\n",
    "class NN(Model):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__(activation)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_dim = self.context_len * 256\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), \n",
    "            self.activation,\n",
    "            nn.Linear(128, self.pred_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        x_flat = ts_embed[:, :self.context_len, :].reshape(x['load'].shape[0], -1)\n",
    "        return self.model(x_flat).unsqueeze(-1)\n",
    "\n",
    "\n",
    "class RNN(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.rnn1, self.rnn2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        rnn1 = nn.RNN(256, 128, batch_first=True)\n",
    "        rnn2 = nn.RNN(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return rnn1, rnn2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.rnn1(ts_embed)\n",
    "        out2, _ = self.rnn2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)\n",
    "\n",
    "class LSTM(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.lstm1, self.lstm2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        lstm1 = nn.LSTM(256, 128, batch_first=True)\n",
    "        lstm2 = nn.LSTM(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return lstm1, lstm2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.lstm1(ts_embed)\n",
    "        out2, _ = self.lstm2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)\n",
    "\n",
    "class GRU(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.gru1, self.gru2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        gru1 = nn.GRU(256, 128, batch_first=True)\n",
    "        gru2 = nn.GRU(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return gru1, gru2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.gru1(ts_embed)\n",
    "        out2, _ = self.gru2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model_name, device, scaler_transform, dataset_name, epochs, train_buildings, test_buildings, activation='relu', optimizer_name='adam', lr=1e-3):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.scaler_transform = scaler_transform\n",
    "        self.dataset_name = dataset_name\n",
    "        self.epochs = epochs\n",
    "        self.train_buildings = train_buildings\n",
    "        self.test_buildings = test_buildings\n",
    "        self.activation = activation\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.lr = lr\n",
    "        self.model = self._load_model()\n",
    "        self.optimizer = self._get_optimizer()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.handler = DataHandler(batch_size=32)\n",
    "        self.path = os.path.join(os.getcwd(), self.dataset_name, self.model_name, self.activation, self.optimizer_name, f'epochs-{self.epochs}')\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "\n",
    "    def _load_model(self):\n",
    "        model_map = {\n",
    "            'NN': NN,\n",
    "            'RNN': RNN,\n",
    "            'LSTM': LSTM,\n",
    "            'GRU': GRU, \n",
    "            'MyNN': MyNN\n",
    "        }\n",
    "        return model_map[self.model_name](activation=self.activation).to(self.device)\n",
    "\n",
    "    def _get_optimizer(self):\n",
    "        opt_map = {\n",
    "            'adam': torch.optim.Adam,\n",
    "            'sgd': torch.optim.SGD,\n",
    "            'adamw': torch.optim.AdamW\n",
    "        }\n",
    "        optimizer_cls = opt_map.get(self.optimizer_name.lower(), torch.optim.Adam)\n",
    "        return optimizer_cls(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        log = []\n",
    "        start_time = time.time()  # Start timer\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0.0\n",
    "            for building_id, building_dataset in self.train_buildings:\n",
    "                dataloader = self.handler.create_dataloader(building_dataset)\n",
    "                for batch in dataloader:\n",
    "                    for key, value in batch.items():\n",
    "                        batch[key] = value.to(self.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    predictions = self.model(batch)\n",
    "                    targets = batch['load'][:, self.model.context_len:, 0]\n",
    "                    loss = self.loss_fn(predictions[:, :, 0], targets)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "            print(f\"[{self.model_name}] Epoch {epoch + 1}: Loss = {total_loss:.4f}\")\n",
    "            log.append({\"epoch\": epoch + 1, \"loss\": total_loss})\n",
    "        train_duration = time.time() - start_time  # End timer\n",
    "        with open(os.path.join(self.path, \"train_loss.json\"), \"w\") as f:\n",
    "             json.dump({\"train_loss\": log, \"train_duration\": train_duration}, f, indent=2)\n",
    "        return train_duration\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        results = {}\n",
    "        mae_total = 0.0\n",
    "        rmse_total = 0.0\n",
    "        r2_total = 0.0\n",
    "        count = 0\n",
    "        for building_id, building_dataset in self.test_buildings:\n",
    "            inverse_transform = building_dataset.datasets[0].load_transform.undo_transform\n",
    "            dataloader = self.handler.create_dataloader(building_dataset)\n",
    "            \n",
    "            target_list = []\n",
    "            prediction_list = []\n",
    "            load_list = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in dataloader:\n",
    "                    for key, value in batch.items():\n",
    "                        batch[key] = value.to(self.device)\n",
    "\n",
    "                    \n",
    "                    predictions = self.model(batch)\n",
    "                    targets = batch['load'][:, self.model.context_len:]\n",
    "                    loads = batch['load'][:, :self.model.context_len]\n",
    "                    \n",
    "                    targets = inverse_transform(targets)\n",
    "                    predictions = inverse_transform(predictions)\n",
    "                    loads = inverse_transform(loads)\n",
    "                    \n",
    "                    prediction_list.append(predictions.detach().cpu())\n",
    "                    target_list.append(targets.detach().cpu())\n",
    "                    load_list.append(loads.detach().cpu())\n",
    "            \n",
    "            predictions_all = torch.cat(prediction_list)\n",
    "            targets_all = torch.cat(target_list)\n",
    "            load_all = torch.cat(load_list)\n",
    "            \n",
    "            mae = torch.abs(predictions_all - targets_all).mean().item()\n",
    "            rmse = torch.sqrt(((predictions_all - targets_all) ** 2).mean()).item()\n",
    "            r2 = 1 - (((predictions_all - targets_all) ** 2).sum() / ((targets_all - targets_all.mean()) ** 2).sum()).item()\n",
    "            mae_total += mae\n",
    "            rmse_total += rmse\n",
    "            r2_total += r2\n",
    "            count += 1\n",
    "            results[building_id] = {\n",
    "                \"load\": load_all.tolist(),\n",
    "                \"predictions\": predictions_all.tolist(),\n",
    "                \"targets\": targets_all.tolist()\n",
    "            }\n",
    "        with open(os.path.join(self.path, \"predictions.json\"), \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        eval_metrics = {\n",
    "            \"mae\": mae_total / count,\n",
    "            \"rmse\": rmse_total / count,\n",
    "            \"r2\": r2_total / count}\n",
    "        with open(os.path.join(self.path, \"evaluate_model.json\"), \"w\") as f:\n",
    "            json.dump(eval_metrics, f, indent=2)\n",
    "        return results, eval_metrics[\"mae\"], eval_metrics[\"rmse\"], eval_metrics[\"r2\"]\n",
    "\n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbb0eb-6087-4f04-b280-18b90fb47aa0",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8e2a82e-c79a-4248-8c5c-c80deb583893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: electricity ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training RNN | Activation: leaky_relu | Optimizer: adamw | Epochs: 2 ---\n",
      "[RNN] MAE: 1540.5220, RMSE: 1774.9586, R²: -254.7830\n",
      "Training Time: 1009.32 seconds\n",
      "\n",
      "--- Training RNN | Activation: leaky_relu | Optimizer: sgd | Epochs: 2 ---\n",
      "[RNN] Epoch 1: Loss = 1314.5254\n",
      "[RNN] Epoch 2: Loss = 394.8243\n",
      "[RNN] MAE: 902.7325, RMSE: 1117.5636, R²: 0.3583\n",
      "Training Time: 1036.64 seconds\n",
      "\n",
      "--- Training RNN | Activation: tanh | Optimizer: adamw | Epochs: 2 ---\n",
      "[RNN] Epoch 1: Loss = 1198.0879\n",
      "[RNN] Epoch 2: Loss = 1234.5671\n",
      "[RNN] MAE: 1697.8887, RMSE: 1939.5182, R²: -369.1338\n",
      "Training Time: 1065.86 seconds\n",
      "\n",
      "--- Training RNN | Activation: tanh | Optimizer: sgd | Epochs: 2 ---\n",
      "[RNN] Epoch 1: Loss = 1204.2811\n",
      "[RNN] Epoch 2: Loss = 307.5832\n",
      "[RNN] MAE: 874.2019, RMSE: 1091.2731, R²: 0.3944\n",
      "Training Time: 1025.81 seconds\n"
     ]
    }
   ],
   "source": [
    "# ------------------- #\n",
    "# ------ Edit ------- #\n",
    "# ------------------- #\n",
    "# dataset_names = [\"ideal\"] # TODO: Provide the dataset name as a list using the format [dataset_name] instead of passing it as a variable\n",
    "# # TODO: Explore at least 30 different combinations using all four models\n",
    "# model_classes = [\"NN\", \"RNN\", \"LSTM\", \"GRU\"]\n",
    "# activations = [\"relu\", \"tanh\", \"leaky_relu\", \"gelu\"]\n",
    "# optimizers = [\"adam\", \"sgd\", \"adamw\"]\n",
    "# epoch_options = [5, 10, 15]\n",
    "dataset_names = [\"electricity\"] # TODO: Provide the dataset name as a list using the format [dataset_name]\n",
    "# TODO: Explore at least 30 different combinations using all four models\n",
    "model_classes = [\"RNN\"]\n",
    "activations = [\"leaky_relu\", \"tanh\"]\n",
    "optimizers = [\"adamw\", \"sgd\"]\n",
    "epoch_options = [2]\n",
    "# ------------------- #\n",
    "# ------ Edit ------- #\n",
    "# ------------------- #\n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #\n",
    "class MyNN(Model):\n",
    "     def __init__(self):\n",
    "         pass\n",
    "os.environ[\"REPO_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/BuildingsBench\"\n",
    "os.environ[\"BUILDINGS_BENCH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset\"\n",
    "os.environ[\"TRANSFORM_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset/metadata/transforms\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "    handler = DataHandler(batch_size=32)\n",
    "    all_buildings = handler.load_dataset(dataset_name, scaler_transform=\"boxcox\")\n",
    "    train_buildings = all_buildings[:int(0.8 * len(all_buildings))]\n",
    "    test_buildings = all_buildings[int(0.8 * len(all_buildings)):]\n",
    "    for model_class in model_classes:\n",
    "        for activation in activations:\n",
    "            for optimizer_name in optimizers:\n",
    "                for epochs in epoch_options:\n",
    "                    print(f\"\\n--- Training {model_class} | Activation: {activation} | Optimizer: {optimizer_name} | Epochs: {epochs} ---\")\n",
    "                    trainer = Trainer(\n",
    "                        model_name=model_class,\n",
    "                        device=device,\n",
    "                        dataset_name=dataset_name,\n",
    "                        epochs=epochs,\n",
    "                        train_buildings=train_buildings,\n",
    "                        test_buildings=test_buildings,\n",
    "                        scaler_transform=\"boxcox\",\n",
    "                        activation=activation,\n",
    "                        optimizer_name=optimizer_name,\n",
    "                        lr=1e-3)\n",
    "                    train_duration = trainer.train()\n",
    "                    results, mae, rmse, r2 = trainer.evaluate()\n",
    "                    print(f\"[{model_class}] MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "                    print(f\"Training Time: {train_duration:.2f} seconds\")\n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# -------------------#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b26a7-98e0-494a-8645-8989aeda4279",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaea9525-3fb2-4c7c-9ef4-989c2542774f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: electricity ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training MyNN | Activation: relu | Optimizer: adam | Epochs: 1 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 88\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Activation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Optimizer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     78\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_class,\n\u001b[1;32m     79\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     optimizer_name\u001b[38;5;241m=\u001b[39moptimizer_name,\n\u001b[1;32m     87\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m train_duration \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m results, mae, rmse, r2 \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, R²: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 204\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m building_id, building_dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_buildings:\n\u001b[1;32m    203\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandler\u001b[38;5;241m.\u001b[39mcreate_dataloader(building_dataset)\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    206\u001b[0m             batch[key] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/torch/utils/data/dataset.py:350\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/pscratch/sd/k/kareem8/BuildingsBenchTutorial/BuildingsBench/buildings_bench/data/datasets.py:72\u001b[0m, in \u001b[0;36mTorchBuildingDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     70\u001b[0m load_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpower\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[seq_ptr\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_len : seq_ptr\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_scaler_transform \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m     load_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_transform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m time_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_transform\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex[seq_ptr\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_len : seq_ptr\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len ])\n\u001b[1;32m     74\u001b[0m latlon_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_latlon\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \n",
      "File \u001b[0;32m/pscratch/sd/k/kareem8/BuildingsBenchTutorial/BuildingsBench/buildings_bench/transforms.py:57\u001b[0m, in \u001b[0;36mBoxCoxTransform.transform\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform a sample via Box-Cox.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mNot ran on the GPU, so input/output are numpy arrays.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    transformed_sample (np.ndarray): of shape (n, 1) or (b,n,1)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m init_shape \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboxcox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(init_shape)\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3112\u001b[0m, in \u001b[0;36mPowerTransformer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   3107\u001b[0m transform_function \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3108\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbox-cox\u001b[39m\u001b[38;5;124m\"\u001b[39m: boxcox,\n\u001b[1;32m   3109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myeo-johnson\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_yeo_johnson_transform,\n\u001b[1;32m   3110\u001b[0m }[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod]\n\u001b[1;32m   3111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, lmbda \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdas_):\n\u001b[0;32m-> 3112\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# hide NaN warnings\u001b[39;00m\n\u001b[1;32m   3113\u001b[0m         X[:, i] \u001b[38;5;241m=\u001b[39m transform_function(X[:, i], lmbda)\n\u001b[1;32m   3115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstandardize:\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/_ufunc_config.py:435\u001b[0m, in \u001b[0;36merrstate.__exit__\u001b[0;34m(self, *exc_info)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _Unspecified:\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moldcall \u001b[38;5;241m=\u001b[39m seterrcall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall)\n\u001b[0;32m--> 435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mexc_info):\n\u001b[1;32m    436\u001b[0m     seterr(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moldstate)\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _Unspecified:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MyNN(Model):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__(activation)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _get_activation(self, activation):\n",
    "        # Map the string to a torch.nn activation; default to ReLU if unknown\n",
    "        name = (activation or \"\").lower()\n",
    "        if name == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        if name == \"tanh\":\n",
    "            return nn.Tanh()\n",
    "        if name == \"leaky_relu\":\n",
    "            return nn.LeakyReLU()\n",
    "        if name == \"gelu\":\n",
    "            return nn.GELU()\n",
    "        return nn.ReLU()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_dim = self.context_len * 256\n",
    "        act = self.activation  # use the already created activation from parent init\n",
    "        return nn.Sequential(\n",
    "            # TODO: Create an input layer using input_dim as the dimension parameter\n",
    "            nn.Linear(input_dim, 512),\n",
    "\n",
    "            # TODO: Add at least three hidden layers\n",
    "            act,\n",
    "            nn.Linear(512, 256),\n",
    "            act,\n",
    "            nn.Linear(256, 128),\n",
    "            act,\n",
    "\n",
    "            # TODO: Create an output layer using self.pred_len as the dimension parameter\n",
    "            nn.Linear(128, self.pred_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        x_flat = ts_embed[:, :self.context_len, :].reshape(x['load'].shape[0], -1)\n",
    "        return self.model(x_flat).unsqueeze(-1)\n",
    "\n",
    "\n",
    "# TODO: Provide the dataset name as a list using the format [dataset_name]\n",
    "dataset_names = [\"electricity\"]\n",
    "\n",
    "# TODO: Explore at least 10 different combinations using MyNN\n",
    "model_classes = [\"MyNN\"]\n",
    "activations = [\"relu\", \"tanh\", \"leaky_relu\", \"gelu\"]\n",
    "optimizers = [\"adam\", \"sgd\", \"adamw\"]\n",
    "epoch_options = [1]\n",
    "\n",
    "\n",
    "# ------------------- #\n",
    "# ------ Edit ------- #\n",
    "# ------------------- #\n",
    "\n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #\n",
    "\n",
    "os.environ[\"REPO_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/BuildingsBench\"\n",
    "os.environ[\"BUILDINGS_BENCH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset\"\n",
    "os.environ[\"TRANSFORM_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset/metadata/transforms\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "    handler = DataHandler(batch_size=32)\n",
    "    all_buildings = handler.load_dataset(dataset_name, scaler_transform=\"boxcox\")\n",
    "    train_buildings = all_buildings[:int(0.8 * len(all_buildings))]\n",
    "    test_buildings = all_buildings[int(0.8 * len(all_buildings)):]\n",
    "    for model_class in model_classes:\n",
    "        for activation in activations:\n",
    "            for optimizer_name in optimizers:\n",
    "                for epochs in epoch_options:\n",
    "                    print(f\"\\n--- Training {model_class} | Activation: {activation} | Optimizer: {optimizer_name} | Epochs: {epochs} ---\")\n",
    "                    trainer = Trainer(\n",
    "                        model_name=model_class,\n",
    "                        device=device,\n",
    "                        dataset_name=dataset_name,\n",
    "                        epochs=epochs,\n",
    "                        train_buildings=train_buildings,\n",
    "                        test_buildings=test_buildings,\n",
    "                        scaler_transform=\"boxcox\",\n",
    "                        activation=activation,\n",
    "                        optimizer_name=optimizer_name,\n",
    "                        lr=1e-3)\n",
    "                    train_duration = trainer.train()\n",
    "                    results, mae, rmse, r2 = trainer.evaluate()\n",
    "                    print(f\"[{model_class}] MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "                    print(f\"Training Time: {train_duration:.2f} seconds\")\n",
    "\n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa88d81-e0f0-4823-967f-e0a09812a756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "775bc2fa-cc25-451c-b87f-2ebb8381e01e",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "`/BuildingsBenchTutorial/Tutorials/Final-Project-Modules/Select-Model.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1ff7ed-c816-46a4-897a-d3e99a599e00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_checkpoint.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"model_checkpoint.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538a3be2-e3b0-4fa7-bdf3-e688a0e3159b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'whi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwhi\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'whi' is not defined"
     ]
    }
   ],
   "source": [
    "whi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ce4f6e-82cd-4859-8f1f-8cf28c1155f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler\t DataLoader\t GRU\t LSTM\t Model\t MyNN\t NN\t Path\t RNN\t \n",
      "TimeSeriesSinusoidalPeriodicEmbedding\t Trainer\t activation\t activations\t all_buildings\t dataset_name\t dataset_names\t device\t epoch_options\t \n",
      "epochs\t handler\t json\t load_torch_dataset\t mae\t mean_absolute_error\t mean_squared_error\t model_class\t model_classes\t \n",
      "model_factory\t nn\t np\t optimizer_name\t optimizers\t os\t r2\t r2_score\t results\t \n",
      "rmse\t test_buildings\t time\t tomli\t torch\t train_buildings\t train_duration\t trainer\t \n"
     ]
    }
   ],
   "source": [
    "who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bda9eb-7543-4be7-964d-a945d6d8aad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: electricity ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training MyNN | Activation: gelu | Optimizer: adamw | Epochs: 1 ---\n",
      "[MyNN] Epoch 1: Loss = 2210.1885\n",
      "[MyNN] MAE: 1062.7734, RMSE: 1347.5334, R²: -14.3558\n",
      "Training Time: 465.28 seconds\n",
      "\n",
      "--- Training MyNN | Activation: gelu | Optimizer: adam | Epochs: 1 ---\n",
      "[MyNN] Epoch 1: Loss = 2158.6339\n",
      "[MyNN] MAE: 6985.6502, RMSE: 79496.0821, R²: -148059679.7577\n",
      "Training Time: 467.71 seconds\n",
      "\n",
      "--- Training MyNN | Activation: gelu | Optimizer: sgd | Epochs: 1 ---\n",
      "[MyNN] Epoch 1: Loss = 5479.2613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:3174: RuntimeWarning: overflow encountered in power\n",
      "  x_inv = (x * lmbda + 1) ** (1 / lmbda)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MyNN] MAE: nan, RMSE: nan, R²: nan\n",
      "Training Time: 459.68 seconds\n",
      "\n",
      "--- Training MyNN | Activation: relu | Optimizer: adamw | Epochs: 1 ---\n"
     ]
    }
   ],
   "source": [
    "# this tests all the models and will figure out the best combo\n",
    "# ------------------- #\n",
    "# ------ Edit ------- #\n",
    "# ------------------- #\n",
    "\n",
    "# Your improved MyNN for fast one-epoch learning\n",
    "class MyNN(Model):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__(activation)\n",
    "        self.act = self._get_activation(activation)\n",
    "        self.model = self._build_model()\n",
    "        self._init_weights()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_dim = self.context_len * 256\n",
    "        h1 = max(128, min(1024, input_dim // 2))\n",
    "        h2 = max(64, min(512,  h1 // 2))\n",
    "        h3 = max(32, min(256,  h2 // 2))\n",
    "        p = 0.15\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, h1), nn.BatchNorm1d(h1), self.act, nn.Dropout(p),\n",
    "            nn.Linear(h1, h2),        nn.BatchNorm1d(h2), self.act, nn.Dropout(p),\n",
    "            nn.Linear(h2, h3),        nn.BatchNorm1d(h3), self.act, nn.Dropout(p),\n",
    "            nn.Linear(h3, self.pred_len)\n",
    "        )\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if isinstance(self.act, (nn.ReLU, nn.LeakyReLU, nn.GELU)):\n",
    "                    nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
    "                elif isinstance(self.act, nn.Tanh):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                else:\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        x_flat = ts_embed[:, :self.context_len, :].reshape(x['load'].shape[0], -1)\n",
    "        return self.model(x_flat).unsqueeze(-1)\n",
    "\n",
    "\n",
    "# Dataset and sweep settings\n",
    "dataset_names = [\"electricity\"]   # REQUIRED\n",
    "model_classes = [\"MyNN\"]\n",
    "activations = [\"gelu\", \"relu\", \"leaky_relu\", \"tanh\"]  # 4 activations\n",
    "optimizers = [\"adamw\", \"adam\", \"sgd\"]                # 3 optimizers\n",
    "epoch_options = [1]                                  # fast training\n",
    "\n",
    "# ------------------- #\n",
    "# --- Run Loop ---    #\n",
    "# ------------------- #\n",
    "\n",
    "os.environ[\"REPO_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/BuildingsBench\"\n",
    "os.environ[\"BUILDINGS_BENCH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset\"\n",
    "os.environ[\"TRANSFORM_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset/metadata/transforms\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "    handler = DataHandler(batch_size=32)\n",
    "    all_buildings = handler.load_dataset(dataset_name, scaler_transform=\"boxcox\")\n",
    "    train_buildings = all_buildings[:int(0.8 * len(all_buildings))]\n",
    "    test_buildings = all_buildings[int(0.8 * len(all_buildings)):]\n",
    "    for model_class in model_classes:\n",
    "        for activation in activations:\n",
    "            for optimizer_name in optimizers:\n",
    "                for epochs in epoch_options:\n",
    "                    print(f\"\\n--- Training {model_class} | Activation: {activation} | Optimizer: {optimizer_name} | Epochs: {epochs} ---\")\n",
    "                    trainer = Trainer(\n",
    "                        model_name=model_class,\n",
    "                        device=device,\n",
    "                        dataset_name=dataset_name,\n",
    "                        epochs=epochs,\n",
    "                        train_buildings=train_buildings,\n",
    "                        test_buildings=test_buildings,\n",
    "                        scaler_transform=\"boxcox\",\n",
    "                        activation=activation,\n",
    "                        optimizer_name=optimizer_name,\n",
    "                        lr=1e-3)\n",
    "                    train_duration = trainer.train()\n",
    "                    results, mae, rmse, r2 = trainer.evaluate()\n",
    "                    print(f\"[{model_class}] MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "                    print(f\"Training Time: {train_duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45f09a44-208b-40c1-8de8-b0dbf5ff4c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: electricity ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training MyNN | Activation: gelu | Optimizer: adamw | Epochs: 1 ---\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 87.12 MiB is free. Process 1555621 has 18.58 GiB memory in use. Process 408374 has 416.00 MiB memory in use. Process 624542 has 416.00 MiB memory in use. Process 2183182 has 18.49 GiB memory in use. Including non-PyTorch memory, this process has 1.37 GiB memory in use. Of the allocated memory 878.89 MiB is allocated by PyTorch, and 7.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epochs \u001b[38;5;129;01min\u001b[39;00m epoch_options:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Activation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Optimizer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_buildings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_buildings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_buildings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_buildings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxcox\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     train_duration \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     80\u001b[0m     results, mae, rmse, r2 \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "Cell \u001b[0;32mIn[2], line 170\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model_name, device, scaler_transform, dataset_name, epochs, train_buildings, test_buildings, activation, optimizer_name, lr)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_name \u001b[38;5;241m=\u001b[39m optimizer_name\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m=\u001b[39m lr\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_optimizer()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "Cell \u001b[0;32mIn[2], line 185\u001b[0m, in \u001b[0;36mTrainer._load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    178\u001b[0m     model_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m'\u001b[39m: NN,\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN\u001b[39m\u001b[38;5;124m'\u001b[39m: RNN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMyNN\u001b[39m\u001b[38;5;124m'\u001b[39m: MyNN\n\u001b[1;32m    184\u001b[0m     }\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 87.12 MiB is free. Process 1555621 has 18.58 GiB memory in use. Process 408374 has 416.00 MiB memory in use. Process 624542 has 416.00 MiB memory in use. Process 2183182 has 18.49 GiB memory in use. Including non-PyTorch memory, this process has 1.37 GiB memory in use. Of the allocated memory 878.89 MiB is allocated by PyTorch, and 7.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# === Fastest single-combo run: MyNN + GELU + AdamW, 1 epoch, electricity ===\n",
    "\n",
    "# Strong MyNN tuned for 1-epoch learning\n",
    "class MyNN(Model):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__(activation)\n",
    "        self.act = self._get_activation(activation)\n",
    "        self.model = self._build_model()\n",
    "        self._init_weights()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_dim = self.context_len * 256\n",
    "        h1 = max(128, min(1024, input_dim // 2))\n",
    "        h2 = max(64,  min(512,  h1 // 2))\n",
    "        h3 = max(32,  min(256,  h2 // 2))\n",
    "        p = 0.15\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, h1), nn.BatchNorm1d(h1), self.act, nn.Dropout(p),\n",
    "            nn.Linear(h1, h2),        nn.BatchNorm1d(h2), self.act, nn.Dropout(p),\n",
    "            nn.Linear(h2, h3),        nn.BatchNorm1d(h3), self.act, nn.Dropout(p),\n",
    "            nn.Linear(h3, self.pred_len)\n",
    "        )\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # Kaiming for ReLU-like (GELU included), Xavier for tanh fallback\n",
    "                if isinstance(self.act, (nn.ReLU, nn.LeakyReLU, nn.GELU)):\n",
    "                    nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
    "                elif isinstance(self.act, nn.Tanh):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                else:\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        x_flat = ts_embed[:, :self.context_len, :].reshape(x['load'].shape[0], -1)\n",
    "        return self.model(x_flat).unsqueeze(-1)\n",
    "\n",
    "\n",
    "# --- Minimal config: 1 combo, 1 epoch ---\n",
    "dataset_names = [\"electricity\"]\n",
    "model_classes = [\"MyNN\"]\n",
    "activations   = [\"gelu\"]\n",
    "optimizers    = [\"adamw\"]\n",
    "epoch_options = [1]\n",
    "\n",
    "# --- Run loop ---\n",
    "os.environ[\"REPO_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/BuildingsBench\"\n",
    "os.environ[\"BUILDINGS_BENCH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset\"\n",
    "os.environ[\"TRANSFORM_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset/metadata/transforms\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "    handler = DataHandler(batch_size=32)\n",
    "    all_buildings = handler.load_dataset(dataset_name, scaler_transform=\"boxcox\")\n",
    "    train_buildings = all_buildings[:int(0.8 * len(all_buildings))]\n",
    "    test_buildings  = all_buildings[int(0.8 * len(all_buildings)):]\n",
    "    for model_class in model_classes:\n",
    "        for activation in activations:\n",
    "            for optimizer_name in optimizers:\n",
    "                for epochs in epoch_options:\n",
    "                    print(f\"\\n--- Training {model_class} | Activation: {activation} | Optimizer: {optimizer_name} | Epochs: {epochs} ---\")\n",
    "                    trainer = Trainer(\n",
    "                        model_name=model_class,\n",
    "                        device=device,\n",
    "                        dataset_name=dataset_name,\n",
    "                        epochs=epochs,\n",
    "                        train_buildings=train_buildings,\n",
    "                        test_buildings=test_buildings,\n",
    "                        scaler_transform=\"boxcox\",\n",
    "                        activation=activation,\n",
    "                        optimizer_name=optimizer_name,\n",
    "                        lr=1e-3\n",
    "                    )\n",
    "                    train_duration = trainer.train()\n",
    "                    results, mae, rmse, r2 = trainer.evaluate()\n",
    "                    print(f\"[{model_class}] MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "                    print(f\"Training Time: {train_duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac17a8ba-3dea-4170-83cd-3a484966d08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: electricity ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/global/homes/k/kareem8/.conda/envs/BBenchEnv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training MyNN | Activation: gelu | Optimizer: adamw | Epochs: 1 ---\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 25.12 MiB is free. Process 1555621 has 18.58 GiB memory in use. Process 408374 has 416.00 MiB memory in use. Process 624542 has 416.00 MiB memory in use. Process 2183182 has 18.49 GiB memory in use. Including non-PyTorch memory, this process has 1.43 GiB memory in use. Of the allocated memory 932.83 MiB is allocated by PyTorch, and 15.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 75\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Activation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Optimizer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     64\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_class,\n\u001b[1;32m     65\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m     74\u001b[0m )\n\u001b[0;32m---> 75\u001b[0m train_duration \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m results, mae, rmse, r2 \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, R²: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 211\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m][:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcontext_len:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    210\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(predictions[:, :, \u001b[38;5;241m0\u001b[39m], targets)\n\u001b[0;32m--> 211\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    213\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/BBenchEnv/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 25.12 MiB is free. Process 1555621 has 18.58 GiB memory in use. Process 408374 has 416.00 MiB memory in use. Process 624542 has 416.00 MiB memory in use. Process 2183182 has 18.49 GiB memory in use. Including non-PyTorch memory, this process has 1.43 GiB memory in use. Of the allocated memory 932.83 MiB is allocated by PyTorch, and 15.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# --- Low-memory MyNN ---\n",
    "class MyNN(Model):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__(activation)\n",
    "        self.act = self._get_activation(activation)\n",
    "        self.model = self._build_model()\n",
    "        self._init_weights()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Smaller hidden sizes to save memory\n",
    "        h1, h2, h3 = 256, 128, 64\n",
    "        p = 0.1\n",
    "        input_dim = self.context_len * 256\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, h1), nn.BatchNorm1d(h1), self.act, nn.Dropout(p),\n",
    "            nn.Linear(h1, h2),        nn.BatchNorm1d(h2), self.act, nn.Dropout(p),\n",
    "            nn.Linear(h2, h3),        nn.BatchNorm1d(h3), self.act, nn.Dropout(p),\n",
    "            nn.Linear(h3, self.pred_len)\n",
    "        )\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if isinstance(self.act, (nn.ReLU, nn.LeakyReLU, nn.GELU)):\n",
    "                    nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
    "                elif isinstance(self.act, nn.Tanh):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                else:\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        x_flat = ts_embed[:, :self.context_len, :].reshape(x['load'].shape[0], -1)\n",
    "        return self.model(x_flat).unsqueeze(-1)\n",
    "\n",
    "\n",
    "# --- Config ---\n",
    "dataset_names = [\"electricity\"]\n",
    "model_classes = [\"MyNN\"]\n",
    "activations   = [\"gelu\"]\n",
    "optimizers    = [\"adamw\"]\n",
    "epoch_options = [1]\n",
    "\n",
    "# --- Run ---\n",
    "os.environ[\"REPO_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/BuildingsBench\"\n",
    "os.environ[\"BUILDINGS_BENCH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset\"\n",
    "os.environ[\"TRANSFORM_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset/metadata/transforms\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "    handler = DataHandler(batch_size=16)  # smaller batch size\n",
    "    all_buildings = handler.load_dataset(dataset_name, scaler_transform=\"boxcox\")\n",
    "    train_buildings = all_buildings[:int(0.8 * len(all_buildings))]\n",
    "    test_buildings  = all_buildings[int(0.8 * len(all_buildings)):]\n",
    "    for model_class in model_classes:\n",
    "        for activation in activations:\n",
    "            for optimizer_name in optimizers:\n",
    "                for epochs in epoch_options:\n",
    "                    print(f\"\\n--- Training {model_class} | Activation: {activation} | Optimizer: {optimizer_name} | Epochs: {epochs} ---\")\n",
    "                    trainer = Trainer(\n",
    "                        model_name=model_class,\n",
    "                        device=device,\n",
    "                        dataset_name=dataset_name,\n",
    "                        epochs=epochs,\n",
    "                        train_buildings=train_buildings,\n",
    "                        test_buildings=test_buildings,\n",
    "                        scaler_transform=\"boxcox\",\n",
    "                        activation=activation,\n",
    "                        optimizer_name=optimizer_name,\n",
    "                        lr=1e-3\n",
    "                    )\n",
    "                    train_duration = trainer.train()\n",
    "                    results, mae, rmse, r2 = trainer.evaluate()\n",
    "                    print(f\"[{model_class}] MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "                    print(f\"Training Time: {train_duration:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernel",
   "language": "python",
   "name": "bbenchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
