{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b16fed-8fa6-4ae7-afcf-d5263fabf1d8",
   "metadata": {},
   "source": [
    "# Train Deep Learning Model\n",
    "\n",
    "This notebook consists of two tasks. For both tasks, you must provide your assigned dataset name in list format, just as you did in the previous exercise.\n",
    "\n",
    "When you train a model, it will print the following evaluation metrics:\n",
    "- `Training loss`\n",
    "- `Training time`\n",
    "- `Mean Absolute Error (MAE)`\n",
    "- `Root Mean Squared Error (RMSE)`\n",
    "- `R² score`\n",
    "\n",
    "Below is an overview of the three metrics:\n",
    "\n",
    "### Evaluation Metrics and Their Desired Trends\n",
    "\n",
    "| Metric             | Definition                                                                                  | Desired Trend                                  |\n",
    "|--------------------|----------------------------------------------------------------------------------------------|------------------------------------------------|\n",
    "| MAE (Mean Absolute Error) | Measures the average absolute difference between predicted and actual values.           | ↓ Decrease — Lower MAE indicates better average accuracy. |\n",
    "| RMSE (Root Mean Squared Error)  | Measures the average of squared differences; penalizes larger errors more heavily.      | ↓ Decrease — Lower RMSE indicates fewer large errors.       |\n",
    "| R² (Coefficient of Determination) | Indicates the proportion of variance explained by the model (maximum = 1.0).        | ↑ Increase — Higher R² means better model fit.             |\n",
    "\n",
    "\n",
    "Please familiarize yourself with the metrics above, as they will be important for completing the analysis in the next tutorial.\n",
    "\n",
    "After model evaluation is complete, a folder will be created in your current directory using the following structure:\n",
    "`dataset_name → model_name → activation_function → optimizer_name → epoch_num`\n",
    "\n",
    "Three JSON files will be generated within this directory structure: \n",
    "1) `predictions.json`\n",
    "2) `evaluation.json`\n",
    "3) `train_loss.json`\n",
    "\n",
    "The predictions.json file stores the input data points used for forecasting, along with the predicted values and corresponding target values. The evaluation.json file contains the evaluation metrics, while the train_loss.json file records the training loss and model training time.\n",
    "\n",
    "## Task 1: Train Predefined Models\n",
    "\n",
    "This task involves training four predefined model architectures using the dataset you have been assigned. These models include:\n",
    "\n",
    "* `NN`\n",
    "* `RNN` (Recurrent Neural Network)\n",
    "* `LSTM` (Long Short-Term Memory)\n",
    "* `GRU` (Gated Recurrent Unit)\n",
    "\n",
    "Note: `LSTM` and `GRU` are specialized types of `RNNs` designed to handle sequence data more effectively.\n",
    "\n",
    "You are not required to train all combinations — there are 125 or more available — but you should train at least 30 different combinations. Make sure these include at least four combinations for each model architecture (`NN`, `RNN`, `LSTM`, and `GRU`) to ensure broad coverage.\n",
    "\n",
    "Example Model Combinations\n",
    "```\n",
    "| Model | Activation Function | Optimizer | Epochs |\n",
    "|-------|---------------------|-----------|--------|\n",
    "| NN    | ReLU                | AdamW     | 10     |\n",
    "| RNN   | Tanh                | SGD       | 5      |\n",
    "| LSTM  | GELU                | Adam      | 10     |\n",
    "| GRU   | Leaky ReLU          | AdamW     | 15     |\n",
    "```\n",
    "\n",
    "The code is configured to run over 125 model combinations, which may take significant time to complete.\n",
    "\n",
    "To speed up development and avoid long runtimes, you can reduce the number of combinations by limiting the range of options.\n",
    "\n",
    "Shrinked Option Set (Example 1)\n",
    "```\n",
    "model_classes = [\"NN\", \"RNN\"]\n",
    "activations = [\"relu\", \"tanh\"]\n",
    "optimizers = [\"adam\"]\n",
    "epoch_options = [5, 10]\n",
    "```\n",
    "\n",
    "Shrinked Option Set (Example 2 — Quick Debug)\n",
    "```\n",
    "model_classes = [\"NN\"]\n",
    "activations = [\"relu\"]\n",
    "optimizers = [\"adam\"]\n",
    "epoch_options = [5]\n",
    "```\n",
    "\n",
    "Note: Do not attempt to train for more than 30 epochs per combination, as this may lead to long execution times and unnecessary resource usage.\n",
    "\n",
    "## Task 2: Build and Train Your Custom Model\n",
    "\n",
    "This task focuses on building and training your own custom model, referred to as `MyNN`.\n",
    "\n",
    "In this task, you will design your own model architecture and evaluate its performance by training it under different settings. You should train at least `10` different combinations using `MyNN`.\n",
    "\n",
    "Example Combinations with `MyNN`\n",
    "```\n",
    "| Model | Activation Function | Optimizer | Epochs |\n",
    "|-------|---------------------|-----------|--------|\n",
    "| MyNN  | ReLU                | AdamW     | 10     |\n",
    "| MyNN  | Tanh                | SGD       | 5      |\n",
    "| MyNN  | GELU                | Adam      | 10     |\n",
    "| MyNN  | Leaky ReLU          | AdamW     | 15     |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e715fda-612f-45ef-aa4e-8a953b5a2d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ------------------- #\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# --- Do Not Edit --- #\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ------------------- #\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from buildings_bench import load_torch_dataset\n",
    "from buildings_bench.models import model_factory\n",
    "\n",
    "import tomli\n",
    "from pathlib import Path\n",
    "import os \n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "class DataHandler:\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def load_dataset(self, dataset_name, scaler_transform):\n",
    "        from buildings_bench import load_torch_dataset\n",
    "        return list(load_torch_dataset(\n",
    "            dataset_name,\n",
    "            apply_scaler_transform=scaler_transform,\n",
    "            scaler_transform_path=Path(os.environ[\"TRANSFORM_PATH\"])\n",
    "        ))\n",
    "\n",
    "    def create_dataloader(self, dataset):\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "class TimeSeriesSinusoidalPeriodicEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([torch.sin(torch.pi * x), torch.cos(torch.pi * x)], dim=2)\n",
    "        return self.linear(x)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    DEFAULT_CONTEXT_LEN = 168\n",
    "    DEFAULT_PRED_LEN = 24\n",
    "\n",
    "    def __init__(self, activation):\n",
    "        super().__init__()\n",
    "        self.context_len = self.DEFAULT_CONTEXT_LEN\n",
    "        self.pred_len = self.DEFAULT_PRED_LEN\n",
    "        self.activation = self._get_activation(activation)\n",
    "        self.embeddings = self._create_embeddings()\n",
    "\n",
    "    def _create_embeddings(self):\n",
    "        return nn.ModuleDict({\n",
    "            'power': nn.Linear(1, 64),\n",
    "            'building': nn.Embedding(2, 32),\n",
    "            'lat': nn.Linear(1, 32),\n",
    "            'lon': nn.Linear(1, 32), \n",
    "            'day_of_year': TimeSeriesSinusoidalPeriodicEmbedding(32),\n",
    "            'day_of_week': TimeSeriesSinusoidalPeriodicEmbedding(32),\n",
    "            'hour_of_day': TimeSeriesSinusoidalPeriodicEmbedding(32)\n",
    "        })\n",
    "\n",
    "    def _get_activation(self, name):\n",
    "        return {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"leaky_relu\": nn.LeakyReLU()\n",
    "        }.get(name.lower(), nn.ReLU())\n",
    "\n",
    "    def _data_pre_process(self, x):\n",
    "        lat = self.embeddings['lat'](x['latitude'])\n",
    "        lon = self.embeddings['lon'](x['longitude'])\n",
    "        btype = self.embeddings['building'](x['building_type'].squeeze(-1))\n",
    "        load = self.embeddings['power'](x['load'])\n",
    "        day_of_year = self.embeddings['day_of_year'](x['day_of_year'])            \n",
    "        day_of_week = self.embeddings['day_of_week'](x['day_of_week'])            \n",
    "        hour_of_day = self.embeddings['hour_of_day'](x['hour_of_day']) \n",
    "        return torch.cat([lat, lon, btype, day_of_year, day_of_week, hour_of_day, load], dim=2)\n",
    "\n",
    "class NN(Model):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__(activation)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_dim = self.context_len * 256\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), \n",
    "            self.activation,\n",
    "            nn.Linear(128, self.pred_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        x_flat = ts_embed[:, :self.context_len, :].reshape(x['load'].shape[0], -1)\n",
    "        return self.model(x_flat).unsqueeze(-1)\n",
    "\n",
    "\n",
    "class RNN(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.rnn1, self.rnn2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        rnn1 = nn.RNN(256, 128, batch_first=True)\n",
    "        rnn2 = nn.RNN(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return rnn1, rnn2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.rnn1(ts_embed)\n",
    "        out2, _ = self.rnn2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)\n",
    "\n",
    "class LSTM(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.lstm1, self.lstm2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        lstm1 = nn.LSTM(256, 128, batch_first=True)\n",
    "        lstm2 = nn.LSTM(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return lstm1, lstm2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.lstm1(ts_embed)\n",
    "        out2, _ = self.lstm2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)\n",
    "\n",
    "class GRU(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.gru1, self.gru2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        gru1 = nn.GRU(256, 128, batch_first=True)\n",
    "        gru2 = nn.GRU(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return gru1, gru2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.gru1(ts_embed)\n",
    "        out2, _ = self.gru2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model_name, device, scaler_transform, dataset_name, epochs, train_buildings, test_buildings, activation='relu', optimizer_name='adam', lr=1e-3):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.scaler_transform = scaler_transform\n",
    "        self.dataset_name = dataset_name\n",
    "        self.epochs = epochs\n",
    "        self.train_buildings = train_buildings\n",
    "        self.test_buildings = test_buildings\n",
    "        self.activation = activation\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.lr = lr\n",
    "        self.model = self._load_model()\n",
    "        self.optimizer = self._get_optimizer()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.handler = DataHandler(batch_size=32)\n",
    "        self.path = os.path.join(os.getcwd(), self.dataset_name, self.model_name, self.activation, self.optimizer_name, f'epochs-{self.epochs}')\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "\n",
    "    def _load_model(self):\n",
    "        model_map = {\n",
    "            'NN': NN,\n",
    "            'RNN': RNN,\n",
    "            'LSTM': LSTM,\n",
    "            'GRU': GRU, \n",
    "            'MyNN': MyNN\n",
    "        }\n",
    "        return model_map[self.model_name](activation=self.activation).to(self.device)\n",
    "\n",
    "    def _get_optimizer(self):\n",
    "        opt_map = {\n",
    "            'adam': torch.optim.Adam,\n",
    "            'sgd': torch.optim.SGD,\n",
    "            'adamw': torch.optim.AdamW\n",
    "        }\n",
    "        optimizer_cls = opt_map.get(self.optimizer_name.lower(), torch.optim.Adam)\n",
    "        return optimizer_cls(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        log = []\n",
    "        start_time = time.time()  # Start timer\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0.0\n",
    "            for building_id, building_dataset in self.train_buildings:\n",
    "                dataloader = self.handler.create_dataloader(building_dataset)\n",
    "                for batch in dataloader:\n",
    "                    for key, value in batch.items():\n",
    "                        batch[key] = value.to(self.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    predictions = self.model(batch)\n",
    "                    targets = batch['load'][:, self.model.context_len:, 0]\n",
    "                    loss = self.loss_fn(predictions[:, :, 0], targets)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "            print(f\"[{self.model_name}] Epoch {epoch + 1}: Loss = {total_loss:.4f}\")\n",
    "            log.append({\"epoch\": epoch + 1, \"loss\": total_loss})\n",
    "        train_duration = time.time() - start_time  # End timer\n",
    "        with open(os.path.join(self.path, \"train_loss.json\"), \"w\") as f:\n",
    "             json.dump({\"train_loss\": log, \"train_duration\": train_duration}, f, indent=2)\n",
    "        return train_duration\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        results = {}\n",
    "        mae_total = 0.0\n",
    "        rmse_total = 0.0\n",
    "        r2_total = 0.0\n",
    "        count = 0\n",
    "        for building_id, building_dataset in self.test_buildings:\n",
    "            inverse_transform = building_dataset.datasets[0].load_transform.undo_transform\n",
    "            dataloader = self.handler.create_dataloader(building_dataset)\n",
    "            \n",
    "            target_list = []\n",
    "            prediction_list = []\n",
    "            load_list = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in dataloader:\n",
    "                    for key, value in batch.items():\n",
    "                        batch[key] = value.to(self.device)\n",
    "\n",
    "                    \n",
    "                    predictions = self.model(batch)\n",
    "                    targets = batch['load'][:, self.model.context_len:]\n",
    "                    loads = batch['load'][:, :self.model.context_len]\n",
    "                    \n",
    "                    targets = inverse_transform(targets)\n",
    "                    predictions = inverse_transform(predictions)\n",
    "                    loads = inverse_transform(loads)\n",
    "                    \n",
    "                    prediction_list.append(predictions.detach().cpu())\n",
    "                    target_list.append(targets.detach().cpu())\n",
    "                    load_list.append(loads.detach().cpu())\n",
    "            \n",
    "            predictions_all = torch.cat(prediction_list)\n",
    "            targets_all = torch.cat(target_list)\n",
    "            load_all = torch.cat(load_list)\n",
    "            \n",
    "            mae = torch.abs(predictions_all - targets_all).mean().item()\n",
    "            rmse = torch.sqrt(((predictions_all - targets_all) ** 2).mean()).item()\n",
    "            r2 = 1 - (((predictions_all - targets_all) ** 2).sum() / ((targets_all - targets_all.mean()) ** 2).sum()).item()\n",
    "            mae_total += mae\n",
    "            rmse_total += rmse\n",
    "            r2_total += r2\n",
    "            count += 1\n",
    "            results[building_id] = {\n",
    "                \"load\": load_all.tolist(),\n",
    "                \"predictions\": predictions_all.tolist(),\n",
    "                \"targets\": targets_all.tolist()\n",
    "            }\n",
    "        with open(os.path.join(self.path, \"predictions.json\"), \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        eval_metrics = {\n",
    "            \"mae\": mae_total / count,\n",
    "            \"rmse\": rmse_total / count,\n",
    "            \"r2\": r2_total / count}\n",
    "        with open(os.path.join(self.path, \"evaluate_model.json\"), \"w\") as f:\n",
    "            json.dump(eval_metrics, f, indent=2)\n",
    "        return results, eval_metrics[\"mae\"], eval_metrics[\"rmse\"], eval_metrics[\"r2\"]\n",
    "\n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbb0eb-6087-4f04-b280-18b90fb47aa0",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f2a80a-1232-4e76-88b4-520cc0c48e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: ideal ===\n",
      "\n",
      "--- Training NN | Activation: relu | Optimizer: adam | Epochs: 5 ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epochs \u001b[38;5;129;01min\u001b[39;00m epoch_options:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Activation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Optimizer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_buildings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_buildings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_buildings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_buildings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxcox\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     train_duration \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     67\u001b[0m     results, mae, rmse, r2 \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "Cell \u001b[0;32mIn[1], line 170\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model_name, device, scaler_transform, dataset_name, epochs, train_buildings, test_buildings, activation, optimizer_name, lr)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_name \u001b[38;5;241m=\u001b[39m optimizer_name\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m=\u001b[39m lr\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_optimizer()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "Cell \u001b[0;32mIn[1], line 185\u001b[0m, in \u001b[0;36mTrainer._load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    178\u001b[0m     model_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m'\u001b[39m: NN,\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN\u001b[39m\u001b[38;5;124m'\u001b[39m: RNN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMyNN\u001b[39m\u001b[38;5;124m'\u001b[39m: MyNN\n\u001b[1;32m    184\u001b[0m     }\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/BuildingsBenchEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/BuildingsBenchEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/BuildingsBenchEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/BuildingsBenchEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/BuildingsBenchEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- #\n",
    "# ------ Edit ------- #\n",
    "# ------------------- #\n",
    "\n",
    "# dataset_names = [\"ideal\"] # TODO: Provide the dataset name as a list using the format [dataset_name] instead of passing it as a variable\n",
    "# # TODO: Explore at least 30 different combinations using all four models\n",
    "# model_classes = [\"NN\", \"RNN\", \"LSTM\", \"GRU\"]\n",
    "# activations = [\"relu\", \"tanh\", \"leaky_relu\", \"gelu\"]\n",
    "# optimizers = [\"adam\", \"sgd\", \"adamw\"]\n",
    "# epoch_options = [5, 10, 15]\n",
    "\n",
    "dataset_names = [\"ideal\"] # TODO: Provide the dataset name as a list using the format [dataset_name]\n",
    "# TODO: Explore at least 30 different combinations using all four models\n",
    "# model_classes = [\"NN\"]\n",
    "# activations = [\"relu\"]\n",
    "# optimizers = [\"adam\"]\n",
    "# epoch_options = [1]\n",
    "\n",
    "# 30 diff combinations\n",
    "model_classes = [\"NN\", \"RNN\", \"LSTM\", \"GRU\"]\n",
    "activations = [\"relu\", \"tanh\", \"leaky_relu\", \"gelu\"]\n",
    "optimizers = [\"adam\", \"sgd\", \"adamw\"]\n",
    "epoch_options = [5, 10, 15]\n",
    "\n",
    "\n",
    "# ------------------- #\n",
    "# ------ Edit ------- #\n",
    "# ------------------- #\n",
    "\n",
    "\n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #\n",
    "\n",
    "class MyNN(Model):\n",
    "     def __init__(self):\n",
    "         pass\n",
    "\n",
    "os.environ[\"REPO_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/BuildingsBench\"\n",
    "os.environ[\"BUILDINGS_BENCH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset\"\n",
    "os.environ[\"TRANSFORM_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset/metadata/transforms\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "    handler = DataHandler(batch_size=32)\n",
    "    all_buildings = handler.load_dataset(dataset_name, scaler_transform=\"boxcox\")\n",
    "    train_buildings = all_buildings[:int(0.8 * len(all_buildings))]\n",
    "    test_buildings = all_buildings[int(0.8 * len(all_buildings)):]\n",
    "    for model_class in model_classes:\n",
    "        for activation in activations:\n",
    "            for optimizer_name in optimizers:\n",
    "                for epochs in epoch_options:\n",
    "                    print(f\"\\n--- Training {model_class} | Activation: {activation} | Optimizer: {optimizer_name} | Epochs: {epochs} ---\")\n",
    "                    trainer = Trainer(\n",
    "                        model_name=model_class,\n",
    "                        device=device,\n",
    "                        dataset_name=dataset_name,\n",
    "                        epochs=epochs,\n",
    "                        train_buildings=train_buildings,\n",
    "                        test_buildings=test_buildings,\n",
    "                        scaler_transform=\"boxcox\",\n",
    "                        activation=activation,\n",
    "                        optimizer_name=optimizer_name,\n",
    "                        lr=1e-3)\n",
    "                    train_duration = trainer.train()\n",
    "                    results, mae, rmse, r2 = trainer.evaluate()\n",
    "                    print(f\"[{model_class}] MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "                    print(f\"Training Time: {train_duration:.2f} seconds\")\n",
    "                    \n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b26a7-98e0-494a-8645-8989aeda4279",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea9525-3fb2-4c7c-9ef4-989c2542774f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------- #\n",
    "# ------ Edit ------- #\n",
    "# ------------------- #\n",
    "\n",
    "class MyNN(Model):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__(activation)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_dim = self.context_len * 256\n",
    "        return nn.Sequential(\n",
    "        # TODO: Create an input layer using input_dim as the dimension parameter\n",
    "        # TODO: Add at least three hidden layers\n",
    "        # TODO: Create an output layer using self.pred_len as the dimension parameter\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        x_flat = ts_embed[:, :self.context_len, :].reshape(x['load'].shape[0], -1)\n",
    "        return self.model(x_flat).unsqueeze(-1)\n",
    "\n",
    "\n",
    "dataset_names =  # TODO: Provide the dataset name as a list using the format [dataset_name]\n",
    "# TODO: Explore at least 10 different combinations using MyNN\n",
    "model_classes = [\"MyNN\"]\n",
    "activations = [\"relu\", \"tanh\", \"leaky_relu\", \"gelu\"]\n",
    "optimizers = [\"adam\", \"sgd\", \"adamw\"]\n",
    "epoch_options = [5, 10, 15]\n",
    "\n",
    "\n",
    "# ------------------- #\n",
    "# ------ Edit ------- #\n",
    "# ------------------- #\n",
    "\n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #\n",
    "\n",
    "os.environ[\"REPO_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/BuildingsBench\"\n",
    "os.environ[\"BUILDINGS_BENCH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset\"\n",
    "os.environ[\"TRANSFORM_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset/metadata/transforms\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "    handler = DataHandler(batch_size=32)\n",
    "    all_buildings = handler.load_dataset(dataset_name, scaler_transform=\"boxcox\")\n",
    "    train_buildings = all_buildings[:int(0.8 * len(all_buildings))]\n",
    "    test_buildings = all_buildings[int(0.8 * len(all_buildings)):]\n",
    "    for model_class in model_classes:\n",
    "        for activation in activations:\n",
    "            for optimizer_name in optimizers:\n",
    "                for epochs in epoch_options:\n",
    "                    print(f\"\\n--- Training {model_class} | Activation: {activation} | Optimizer: {optimizer_name} | Epochs: {epochs} ---\")\n",
    "                    trainer = Trainer(\n",
    "                        model_name=model_class,\n",
    "                        device=device,\n",
    "                        dataset_name=dataset_name,\n",
    "                        epochs=epochs,\n",
    "                        train_buildings=train_buildings,\n",
    "                        test_buildings=test_buildings,\n",
    "                        scaler_transform=\"boxcox\",\n",
    "                        activation=activation,\n",
    "                        optimizer_name=optimizer_name,\n",
    "                        lr=1e-3)\n",
    "                    train_duration = trainer.train()\n",
    "                    results, mae, rmse, r2 = trainer.evaluate()\n",
    "                    print(f\"[{model_class}] MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "                    print(f\"Training Time: {train_duration:.2f} seconds\")\n",
    "\n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775bc2fa-cc25-451c-b87f-2ebb8381e01e",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "`/BuildingsBenchTutorial/Tutorials/Final-Project-Modules/Select-Model.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BuildingsBenchKernel-New",
   "language": "python",
   "name": "buildingsbenchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
