{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b16fed-8fa6-4ae7-afcf-d5263fabf1d8",
   "metadata": {},
   "source": [
    "# Train Deep Learning Model\n",
    "\n",
    "This notebook consists of two tasks. For both tasks, you must provide your assigned dataset name in list format, just as you did in the previous exercise.\n",
    "\n",
    "When you train a model, it will print the following evaluation metrics:\n",
    "- Training loss\n",
    "- Training time\n",
    "- Mean Absolute Error (MAE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- R² score (Coefficient of Determination)\n",
    "\n",
    "You have already used MSE in the Intro-NN.ipynb notebook. Below is an overview of the three main metrics:\n",
    "### Evaluation Metrics and Their Desired Trends\n",
    "\n",
    "| Metric             | Definition                                                                                  | Desired Trend                                  |\n",
    "|--------------------|----------------------------------------------------------------------------------------------|------------------------------------------------|\n",
    "| MAE (Mean Absolute Error) | Measures the average absolute difference between predicted and actual values.           | ↓ Decrease — Lower MAE indicates better average accuracy. |\n",
    "| MSE (Mean Squared Error)  | Measures the average of squared differences; penalizes larger errors more heavily.      | ↓ Decrease — Lower MSE indicates fewer large errors.       |\n",
    "| R² (Coefficient of Determination) | Indicates the proportion of variance explained by the model (maximum = 1.0).        | ↑ Increase — Higher R² means better model fit.             |\n",
    "\n",
    "\n",
    "Please familiarize yourself with the metrics above, as they will be important for completing the analysis in the next tutorial.\n",
    "\n",
    "After model evaluation is complete, a folder will be created in your current directory using the following structure:\n",
    "`dataset_name → model_name → activation_function → optimizer_name → epoch_num`\n",
    "\n",
    "Three JSON files will be generated within this directory structure: \n",
    "1) predictions.json\n",
    "2) evaluation.json\n",
    "3) train_loss.json.\n",
    "\n",
    "The predictions.json file stores the input data points used for forecasting, along with the predicted values and corresponding target values. The evaluation.json file contains the evaluation metrics, while the train_loss.json file records the training loss and model training time.\n",
    "\n",
    "## Task 1: Train Predefined Models\n",
    "\n",
    "This task involves training four predefined model architectures using the dataset you have been assigned. These models include:\n",
    "\n",
    "* NN\n",
    "* RNN (Recurrent Neural Network)\n",
    "* LSTM (Long Short-Term Memory)\n",
    "* GRU (Gated Recurrent Unit)\n",
    "\n",
    "Note: LSTM and GRU are specialized types of RNNs designed to handle sequence data more effectively.\n",
    "\n",
    "You are not required to train all combinations — there are 125 or more available — but you should train at least 30 different combinations. Make sure these include at least four combinations for each model architecture (NN, RNN, LSTM, and GRU) to ensure broad coverage.\n",
    "\n",
    "Example Model Combinations\n",
    "```\n",
    "| Model | Activation Function | Optimizer | Epochs |\n",
    "|-------|---------------------|-----------|--------|\n",
    "| NN    | ReLU                | AdamW     | 10     |\n",
    "| RNN   | Tanh                | SGD       | 5      |\n",
    "| LSTM  | GELU                | Adam      | 10     |\n",
    "| GRU   | Leaky ReLU          | AdamW     | 15     |\n",
    "```\n",
    "\n",
    "The code is configured to run over 125 model combinations, which may take significant time to complete.\n",
    "\n",
    "To speed up development and avoid long runtimes, you can reduce the number of combinations by limiting the range of options.\n",
    "\n",
    "Shrinked Option Set (Example 1)\n",
    "```\n",
    "model_classes = [\"NN\", \"RNN\"]\n",
    "activations = [\"relu\", \"tanh\"]\n",
    "optimizers = [\"adam\"]\n",
    "epoch_options = [5, 10]\n",
    "```\n",
    "\n",
    "Shrinked Option Set (Example 2 — Quick Debug)\n",
    "```\n",
    "model_classes = [\"NN\"]\n",
    "activations = [\"relu\"]\n",
    "optimizers = [\"adam\"]\n",
    "epoch_options = [5]\n",
    "```\n",
    "\n",
    "Note: Do not attempt to train for more than 30 epochs per combination, as this may lead to long execution times and unnecessary resource usage.\n",
    "\n",
    "## Task 2: Build and Train Your Custom Model\n",
    "\n",
    "This task focuses on building and training your own custom model, referred to as MyNN.\n",
    "\n",
    "In this task, you will design your own model architecture and evaluate its performance by training it under different settings. You should train at least 10 different combinations using MyNN.\n",
    "\n",
    "Example Combinations with MyNN\n",
    "```\n",
    "| Model | Activation Function | Optimizer | Epochs |\n",
    "|-------|---------------------|-----------|--------|\n",
    "| MyNN  | ReLU                | AdamW     | 10     |\n",
    "| MyNN  | Tanh                | SGD       | 5      |\n",
    "| MyNN  | GELU                | Adam      | 10     |\n",
    "| MyNN  | Leaky ReLU          | AdamW     | 15     |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e715fda-612f-45ef-aa4e-8a953b5a2d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/n/nrushad/.conda/envs/BuildingsBenchEnv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from buildings_bench import load_torch_dataset\n",
    "from buildings_bench.models import model_factory\n",
    "\n",
    "import tomli\n",
    "from pathlib import Path\n",
    "import os \n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "class DataHandler:\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def load_dataset(self, dataset_name, scaler_transform):\n",
    "        from buildings_bench import load_torch_dataset\n",
    "        return list(load_torch_dataset(\n",
    "            dataset_name,\n",
    "            apply_scaler_transform=scaler_transform,\n",
    "            scaler_transform_path=Path(os.environ[\"TRANSFORM_PATH\"])\n",
    "        ))\n",
    "\n",
    "    def create_dataloader(self, dataset):\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "class TimeSeriesSinusoidalPeriodicEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([torch.sin(torch.pi * x), torch.cos(torch.pi * x)], dim=2)\n",
    "        return self.linear(x)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    DEFAULT_CONTEXT_LEN = 168\n",
    "    DEFAULT_PRED_LEN = 24\n",
    "\n",
    "    def __init__(self, activation):\n",
    "        super().__init__()\n",
    "        self.context_len = self.DEFAULT_CONTEXT_LEN\n",
    "        self.pred_len = self.DEFAULT_PRED_LEN\n",
    "        self.activation = self._get_activation(activation)\n",
    "        self.embeddings = self._create_embeddings()\n",
    "\n",
    "    def _create_embeddings(self):\n",
    "        return nn.ModuleDict({\n",
    "            'power': nn.Linear(1, 64),\n",
    "            'building': nn.Embedding(2, 32),\n",
    "            'lat': nn.Linear(1, 32),\n",
    "            'lon': nn.Linear(1, 32), \n",
    "            'day_of_year': TimeSeriesSinusoidalPeriodicEmbedding(32),\n",
    "            'day_of_week': TimeSeriesSinusoidalPeriodicEmbedding(32),\n",
    "            'hour_of_day': TimeSeriesSinusoidalPeriodicEmbedding(32)\n",
    "        })\n",
    "\n",
    "    def _get_activation(self, name):\n",
    "        return {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"leaky_relu\": nn.LeakyReLU()\n",
    "        }.get(name.lower(), nn.ReLU())\n",
    "\n",
    "    def _data_pre_process(self, x):\n",
    "        lat = self.embeddings['lat'](x['latitude'])\n",
    "        lon = self.embeddings['lon'](x['longitude'])\n",
    "        btype = self.embeddings['building'](x['building_type'].squeeze(-1))\n",
    "        load = self.embeddings['power'](x['load'])\n",
    "        day_of_year = self.embeddings['day_of_year'](x['day_of_year'])            \n",
    "        day_of_week = self.embeddings['day_of_week'](x['day_of_week'])            \n",
    "        hour_of_day = self.embeddings['hour_of_day'](x['hour_of_day']) \n",
    "        return torch.cat([lat, lon, btype, day_of_year, day_of_week, hour_of_day, load], dim=2)\n",
    "\n",
    "class NN(Model):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__(activation)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_dim = self.context_len * 256\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), \n",
    "            self.activation,\n",
    "            nn.Linear(128, self.pred_len)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        x_flat = ts_embed[:, :self.context_len, :].reshape(x['load'].shape[0], -1)\n",
    "        return self.model(x_flat).unsqueeze(-1)\n",
    "\n",
    "\n",
    "class RNN(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.rnn1, self.rnn2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        rnn1 = nn.RNN(256, 128, batch_first=True)\n",
    "        rnn2 = nn.RNN(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return rnn1, rnn2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.rnn1(ts_embed)\n",
    "        out2, _ = self.rnn2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)\n",
    "\n",
    "class LSTM(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.lstm1, self.lstm2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        lstm1 = nn.LSTM(256, 128, batch_first=True)\n",
    "        lstm2 = nn.LSTM(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return lstm1, lstm2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.lstm1(ts_embed)\n",
    "        out2, _ = self.lstm2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)\n",
    "\n",
    "class GRU(Model):\n",
    "    def __init__(self, activation=\"relu\"):\n",
    "        super().__init__(activation)\n",
    "        self.gru1, self.gru2, self.output_layer = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        gru1 = nn.GRU(256, 128, batch_first=True)\n",
    "        gru2 = nn.GRU(128, 128, batch_first=True)\n",
    "        output_layer = nn.Linear(128, self.pred_len)\n",
    "        return gru1, gru2, output_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        out1, _ = self.gru1(ts_embed)\n",
    "        out2, _ = self.gru2(out1)\n",
    "        last_hidden = self.activation(out2[:, -1, :])\n",
    "        return self.output_layer(last_hidden).unsqueeze(-1)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model_name, device, scaler_transform, dataset_name, epochs, train_buildings, test_buildings, activation='relu', optimizer_name='adam', lr=1e-3):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.scaler_transform = scaler_transform\n",
    "        self.dataset_name = dataset_name\n",
    "        self.epochs = epochs\n",
    "        self.train_buildings = train_buildings\n",
    "        self.test_buildings = test_buildings\n",
    "        self.activation = activation\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.lr = lr\n",
    "        self.model = self._load_model()\n",
    "        self.optimizer = self._get_optimizer()\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.handler = DataHandler(batch_size=32)\n",
    "        self.path = os.path.join(os.getcwd(), dataset_name, model_name, activation, optimizer_name, f'epochs-{epochs}')\n",
    "        os.makedirs(self.path, exist_ok=True)\n",
    "\n",
    "    def _load_model(self):\n",
    "        model_map = {\n",
    "            'NN': NN,\n",
    "            'RNN': RNN,\n",
    "            'LSTM': LSTM,\n",
    "            'GRU': GRU, \n",
    "            'MyNN': MyNN\n",
    "        }\n",
    "        return model_map[self.model_name](activation=self.activation).to(self.device)\n",
    "\n",
    "    def _get_optimizer(self):\n",
    "        opt_map = {\n",
    "            'adam': torch.optim.Adam,\n",
    "            'sgd': torch.optim.SGD,\n",
    "            'adamw': torch.optim.AdamW\n",
    "        }\n",
    "        optimizer_cls = opt_map.get(self.optimizer_name.lower(), torch.optim.Adam)\n",
    "        return optimizer_cls(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        log = []\n",
    "        start_time = time.time()  # Start timer\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0.0\n",
    "            for building_id, building_dataset in self.train_buildings:\n",
    "                dataloader = self.handler.create_dataloader(building_dataset)\n",
    "                for batch in dataloader:\n",
    "                    for key, value in batch.items():\n",
    "                        batch[key] = value.to(self.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    predictions = self.model(batch)\n",
    "                    targets = batch['load'][:, self.model.context_len:, 0]\n",
    "                    loss = self.loss_fn(predictions[:, :, 0], targets)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "            print(f\"[{self.model_name}] Epoch {epoch + 1}: Loss = {total_loss:.4f}\")\n",
    "            log.append({\"epoch\": epoch + 1, \"loss\": total_loss})\n",
    "        train_duration = time.time() - start_time  # End timer\n",
    "        with open(os.path.join(self.path, \"train_loss.json\"), \"w\") as f:\n",
    "             json.dump({\"train_loss\": log, \"train_duration\": train_duration}, f, indent=2)\n",
    "        return train_duration\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        results = {}\n",
    "        mae_total = 0.0\n",
    "        rmse_total = 0.0\n",
    "        r2_total = 0.0\n",
    "        count = 0\n",
    "        for building_id, building_dataset in self.test_buildings:\n",
    "            inverse_transform = building_dataset.datasets[0].load_transform.undo_transform\n",
    "            dataloader = self.handler.create_dataloader(building_dataset)\n",
    "            \n",
    "            target_list = []\n",
    "            prediction_list = []\n",
    "            load_list = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in dataloader:\n",
    "                    for key, value in batch.items():\n",
    "                        batch[key] = value.to(self.device)\n",
    "\n",
    "                    \n",
    "                    predictions = self.model(batch)\n",
    "                    targets = batch['load'][:, self.model.context_len:]\n",
    "                    loads = batch['load'][:, :self.model.context_len]\n",
    "                    \n",
    "                    targets = inverse_transform(targets)\n",
    "                    predictions = inverse_transform(predictions)\n",
    "                    loads = inverse_transform(loads)\n",
    "                    \n",
    "                    prediction_list.append(predictions.detach().cpu())\n",
    "                    target_list.append(targets.detach().cpu())\n",
    "                    load_list.append(loads.detach().cpu())\n",
    "            \n",
    "            predictions_all = torch.cat(prediction_list)\n",
    "            targets_all = torch.cat(target_list)\n",
    "            load_all = torch.cat(load_list)\n",
    "            \n",
    "            mae = torch.abs(predictions_all - targets_all).mean().item()\n",
    "            rmse = torch.sqrt(((predictions_all - targets_all) ** 2).mean()).item()\n",
    "            r2 = 1 - (((predictions_all - targets_all) ** 2).sum() / ((targets_all - targets_all.mean()) ** 2).sum()).item()\n",
    "            mae_total += mae\n",
    "            rmse_total += rmse\n",
    "            r2_total += r2\n",
    "            count += 1\n",
    "            results[building_id] = {\n",
    "                \"load\": load_all.tolist(),\n",
    "                \"predictions\": predictions_all.tolist(),\n",
    "                \"targets\": targets_all.tolist()\n",
    "            }\n",
    "        with open(os.path.join(self.path, \"predictions.json\"), \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        eval_metrics = {\n",
    "            \"mae\": mae_total / count,\n",
    "            \"rmse\": rmse_total / count,\n",
    "            \"r2\": r2_total / count}\n",
    "        with open(os.path.join(self.path, \"evaluate_model.json\"), \"w\") as f:\n",
    "            json.dump(eval_metrics, f, indent=2)\n",
    "        return results, eval_metrics[\"mae\"], eval_metrics[\"rmse\"], eval_metrics[\"r2\"]\n",
    "\n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbb0eb-6087-4f04-b280-18b90fb47aa0",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f2a80a-1232-4e76-88b4-520cc0c48e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: ideal ===\n",
      "\n",
      "--- Training NN | Activation: relu | Optimizer: adam | Epochs: 1 ---\n",
      "[NN] Epoch 1: Loss = 3056.7305\n",
      "[NN] MAE: 0.5213, RMSE: 0.5768, R²: -3.6301\n",
      "Training Time: 47.06 seconds\n"
     ]
    }
   ],
   "source": [
    "# ------------------- #\n",
    "# ------ Edit ------- #\n",
    "# ------------------- #\n",
    "\n",
    "# dataset_names = [\"ideal\"] # TODO: Provide the dataset name as a list using the format [dataset_name] instead of passing it as a variable\n",
    "# # TODO: Explore at least 30 different combinations using all four models\n",
    "# model_classes = [\"NN\", \"RNN\", \"LSTM\", \"GRU\"]\n",
    "# activations = [\"relu\", \"tanh\", \"leaky_relu\", \"gelu\"]\n",
    "# optimizers = [\"adam\", \"sgd\", \"adamw\"]\n",
    "# epoch_options = [5, 10, 15]\n",
    "\n",
    "dataset_names = [\"ideal\"] # TODO: Provide the dataset name as a list using the format [dataset_name]\n",
    "# TODO: Explore at least 30 different combinations using all four models\n",
    "model_classes = [\"NN\"]\n",
    "activations = [\"relu\"]\n",
    "optimizers = [\"adam\"]\n",
    "epoch_options = [1]\n",
    "\n",
    "# ------------------- #\n",
    "# ------ Edit ------- #\n",
    "# ------------------- #\n",
    "\n",
    "\n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #\n",
    "\n",
    "class MyNN(Model):\n",
    "     def __init__(self):\n",
    "         pass\n",
    "\n",
    "os.environ[\"REPO_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/BuildingsBench\"\n",
    "os.environ[\"BUILDINGS_BENCH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset\"\n",
    "os.environ[\"TRANSFORM_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset/metadata/transforms\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "    handler = DataHandler(batch_size=32)\n",
    "    all_buildings = handler.load_dataset(dataset_name, scaler_transform=\"boxcox\")\n",
    "    train_buildings = all_buildings[:int(0.8 * len(all_buildings))]\n",
    "    test_buildings = all_buildings[int(0.8 * len(all_buildings)):]\n",
    "    for model_class in model_classes:\n",
    "        for activation in activations:\n",
    "            for optimizer_name in optimizers:\n",
    "                for epochs in epoch_options:\n",
    "                    print(f\"\\n--- Training {model_class} | Activation: {activation} | Optimizer: {optimizer_name} | Epochs: {epochs} ---\")\n",
    "                    trainer = Trainer(\n",
    "                        model_name=model_class,\n",
    "                        device=device,\n",
    "                        dataset_name=dataset_name,\n",
    "                        epochs=epochs,\n",
    "                        train_buildings=train_buildings,\n",
    "                        test_buildings=test_buildings,\n",
    "                        scaler_transform=\"boxcox\",\n",
    "                        activation=activation,\n",
    "                        optimizer_name=optimizer_name,\n",
    "                        lr=1e-3)\n",
    "                    train_duration = trainer.train()\n",
    "                    results, mae, rmse, r2 = trainer.evaluate()\n",
    "                    print(f\"[{model_class}] MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "                    print(f\"Training Time: {train_duration:.2f} seconds\")\n",
    "                    \n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b26a7-98e0-494a-8645-8989aeda4279",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea9525-3fb2-4c7c-9ef4-989c2542774f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------- #\n",
    "# ------ Edit ------- #\n",
    "# ------------------- #\n",
    "\n",
    "class MyNN(Model):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__(activation)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_dim = self.context_len * 256\n",
    "        return nn.Sequential(\n",
    "        # TODO: Create an input layer using input_dim as the dimension parameter\n",
    "        # TODO: Add at least three hidden layers\n",
    "        # TODO: Create an output layer using self.pred_len as the dimension parameter\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ts_embed = self._data_pre_process(x)\n",
    "        x_flat = ts_embed[:, :self.context_len, :].reshape(x['load'].shape[0], -1)\n",
    "        return self.model(x_flat).unsqueeze(-1)\n",
    "\n",
    "\n",
    "dataset_names =  # TODO: Provide the dataset name as a list using the format [dataset_name]\n",
    "# TODO: Explore at least 10 different combinations using MyNN\n",
    "model_classes = [\"MyNN\"]\n",
    "activations = [\"relu\", \"tanh\", \"leaky_relu\", \"gelu\"]\n",
    "optimizers = [\"adam\", \"sgd\", \"adamw\"]\n",
    "epoch_options = [5, 10, 15]\n",
    "\n",
    "\n",
    "# ------------------- #\n",
    "# ------ Edit ------- #\n",
    "# ------------------- #\n",
    "\n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #\n",
    "\n",
    "os.environ[\"REPO_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/BuildingsBench\"\n",
    "os.environ[\"BUILDINGS_BENCH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset\"\n",
    "os.environ[\"TRANSFORM_PATH\"] = \"/global/cfs/cdirs/m4388/Project4/Dataset/metadata/transforms\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"\\n=== Dataset: {dataset_name} ===\")\n",
    "    handler = DataHandler(batch_size=32)\n",
    "    all_buildings = handler.load_dataset(dataset_name, scaler_transform=\"boxcox\")\n",
    "    train_buildings = all_buildings[:int(0.8 * len(all_buildings))]\n",
    "    test_buildings = all_buildings[int(0.8 * len(all_buildings)):]\n",
    "    for model_class in model_classes:\n",
    "        for activation in activations:\n",
    "            for optimizer_name in optimizers:\n",
    "                for epochs in epoch_options:\n",
    "                    print(f\"\\n--- Training {model_class} | Activation: {activation} | Optimizer: {optimizer_name} | Epochs: {epochs} ---\")\n",
    "                    trainer = Trainer(\n",
    "                        model_name=model_class,\n",
    "                        device=device,\n",
    "                        dataset_name=dataset_name,\n",
    "                        epochs=epochs,\n",
    "                        train_buildings=train_buildings,\n",
    "                        test_buildings=test_buildings,\n",
    "                        scaler_transform=\"boxcox\",\n",
    "                        activation=activation,\n",
    "                        optimizer_name=optimizer_name,\n",
    "                        lr=1e-3)\n",
    "                    train_duration = trainer.train()\n",
    "                    results, mae, rmse, r2 = trainer.evaluate()\n",
    "                    print(f\"[{model_class}] MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "                    print(f\"Training Time: {train_duration:.2f} seconds\")\n",
    "\n",
    "# ------------------- #\n",
    "# --- Do Not Edit --- #\n",
    "# ------------------- #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BuildingsBenchKernel",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
